{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- #\n",
    "# 1. Only have a single episode (environment is not reset)\n",
    "# 2. Training is the same as usual SAC\n",
    "# 3. In evaluation mode, compute the average return over a of evaluation episode\n",
    "# 4. Each evaluation episode starts with the next state at the current time step\n",
    "# ----------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std\n",
    "    \n",
    "    def sample(self, state, deterministic=False):\n",
    "        mu, std = self.forward(state)\n",
    "        if deterministic:\n",
    "            action = torch.tanh(mu)\n",
    "            return action * self.max_action, None\n",
    "        else:       \n",
    "            normal = Normal(mu, std)\n",
    "            x = normal.rsample()  # Reparameterization trick\n",
    "            action = torch.tanh(x)\n",
    "            log_prob = normal.log_prob(x) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "            log_prob = log_prob.sum(1, keepdim=True)\n",
    "            return action * self.max_action, log_prob\n",
    "\n",
    "# Q-networks\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        # Q1\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q1 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Q2 (for reducing overestimation bias)\n",
    "        self.fc3 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        \n",
    "        # Q1\n",
    "        q1 = F.relu(self.fc1(x))\n",
    "        q1 = F.relu(self.fc2(q1))\n",
    "        q1 = self.q1(q1)\n",
    "        \n",
    "        # Q2\n",
    "        q2 = F.relu(self.fc3(x))\n",
    "        q2 = F.relu(self.fc4(q2))\n",
    "        q2 = self.q2(q2)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "class SAC:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim, \n",
    "        action_dim, \n",
    "        max_action,\n",
    "        alpha=0.2,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        actor_lr=3e-4,\n",
    "        critic_lr=3e-4,\n",
    "        update_frequency=1,\n",
    "        min_buffer_size=100,\n",
    "        batch_size=64,\n",
    "        buffer_size=10000\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.update_frequency = update_frequency\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        \n",
    "        # Copy parameters\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Experience buffer for mini-batch updates\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        action, _ = self.actor.sample(state, deterministic=deterministic)\n",
    "        return action.detach().numpy().flatten()\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        if len(self.buffer) < self.min_buffer_size:\n",
    "            return\n",
    "            \n",
    "        # Sample from buffer\n",
    "        indices = np.random.choice(len(self.buffer), min(self.batch_size, len(self.buffer)))\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        \n",
    "        state = torch.FloatTensor(np.array(state))\n",
    "        action = torch.FloatTensor(np.array(action))\n",
    "        reward = torch.FloatTensor(np.array(reward).reshape(-1, 1))\n",
    "        next_state = torch.FloatTensor(np.array(next_state))\n",
    "        done = torch.FloatTensor(np.array(done).reshape(-1, 1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Sample next action and its log probability\n",
    "            next_action, next_log_prob = self.actor.sample(next_state)\n",
    "            \n",
    "            # Target Q-values\n",
    "            target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target_q = reward + (1 - done) * self.gamma * (target_q - self.alpha * next_log_prob)\n",
    "            \n",
    "        # Current Q-values\n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor loss calculation\n",
    "        new_action, log_prob = self.actor.sample(state)\n",
    "        q1, q2 = self.critic(state, new_action)\n",
    "        q = torch.min(q1, q2)\n",
    "        actor_loss = (self.alpha * log_prob - q).mean()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1 - self.tau))\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.total_steps += 1\n",
    "        if self.total_steps % self.update_frequency == 0:\n",
    "            self.update_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cis/home/adesilva/miniconda3/envs/prol/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at step 0: Future Return: -4.02\n",
      "Evaluation at step 5000: Future Return: -73.57\n",
      "Evaluation at step 10000: Future Return: -48.90\n",
      "Evaluation at step 15000: Future Return: -137.98\n",
      "Evaluation at step 20000: Future Return: -84.91\n",
      "Evaluation at step 25000: Future Return: -510.43\n",
      "Evaluation at step 30000: Future Return: 821.06\n",
      "Evaluation at step 35000: Future Return: -95.12\n",
      "Evaluation at step 40000: Future Return: -91.84\n",
      "Evaluation at step 45000: Future Return: -101.27\n",
      "Evaluation at step 50000: Future Return: -87.15\n",
      "Evaluation at step 55000: Future Return: -78.99\n",
      "Evaluation at step 60000: Future Return: -212.44\n",
      "Evaluation at step 65000: Future Return: -648.94\n"
     ]
    }
   ],
   "source": [
    "def custom_reset(env, custom_state):\n",
    "    # Reset environment and set custom state\n",
    "    env.reset()  # Optional, some environments require a reset\n",
    "    env.state = custom_state  # Set the state to a predefined value\n",
    "    return env.state\n",
    "\n",
    "def evaluate_policy(agent, current_state, env_name=\"HalfCheetah-v4\", max_future_steps=1000):\n",
    "    \"\"\"Compute the cumulative reward of following the current policy in the future from the current state onwards \n",
    "    for a fixed number of steps. This is akin to prospective reward.\n",
    "    \"\"\"\n",
    "    eval_env = gym.make(env_name)\n",
    "    cum_reward = 0.\n",
    "    state = custom_reset(eval_env, current_state)\n",
    "    for _ in range(max_future_steps):\n",
    "        action = agent.select_action(state, deterministic=True)\n",
    "        state, reward, _, _, _ = eval_env.step(action)\n",
    "        cum_reward += reward\n",
    "    return cum_reward\n",
    "\n",
    "def run_online_learning(env_name=\"HalfCheetah-v4\", max_steps=1000000, eval_freq=5000):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = SAC(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        max_action=max_action,\n",
    "        update_frequency=1,  # Update after each step for truly online learning\n",
    "        min_buffer_size=100  # Start learning after collecting 100 samples\n",
    "    )\n",
    "    \n",
    "    # Initial state\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Logging variables\n",
    "    rewards = []\n",
    "    avg_speeds = []\n",
    "    current_speed_window = []\n",
    "    evaluations = []\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"runs/my_experiment\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, _, _, _ = env.step(action)\n",
    "        done = False # because the episode never ends\n",
    "        \n",
    "        # Store transition\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Track speed (x-velocity is typically the relevant metric for half-cheetah)\n",
    "        current_speed_window.append(next_state[8])  # Assuming index 8 is x-velocity\n",
    "        if len(current_speed_window) > 100:\n",
    "            current_speed_window.pop(0)\n",
    "        \n",
    "        # Log performance metrics every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            rewards.append(episode_reward)\n",
    "            avg_speed = np.mean(current_speed_window) if current_speed_window else 0\n",
    "            avg_speeds.append(avg_speed)\n",
    "            # print(f\"Step: {step}, Avg. Reward: {episode_reward/(step+1):.2f}, Avg. Speed: {avg_speed:.2f}\")\n",
    "        \n",
    "        # Evaluate policy periodically\n",
    "        if step % eval_freq == 0:\n",
    "            future_return = evaluate_policy(agent, state, env_name)\n",
    "            evaluations.append(future_return)\n",
    "            print(f\"Evaluation at step {step}: Future Return: {future_return:.2f}\")\n",
    "            writer.add_scalar(\"Future Return\", future_return, step)\n",
    "\n",
    "    writer.close()        \n",
    "    return rewards, avg_speeds, evaluations, agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rewards, speeds, evaluations, agent = run_online_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "methods = [\"sac_metrics.json\", \"online_sac_metrics.json\"]\n",
    "\n",
    "for method in methods:\n",
    "    with open(f'{method}.json', 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    evals = metrics['evaluations']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
