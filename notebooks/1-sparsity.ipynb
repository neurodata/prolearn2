{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessConfig:\n",
    "    \"\"\"configure the data generating process\"\"\"\n",
    "    period: int = 40\n",
    "    seq_len: int = 10000\n",
    "    num_seeds: int = 3 \n",
    "    ùúÜ: float = 5.0 # poisson rate (number of samples received within a unit time interval)\n",
    "    p: float = 0.0 # sparse rate (probability of not receiving any samples at a given time)\n",
    "\n",
    "\n",
    "class DataGeneratingProcess:\n",
    "    \"\"\"data generating process\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        self.seq_len = cfg.seq_len\n",
    "        self.num_seeds = cfg.num_seeds\n",
    "        self.period = cfg.period\n",
    "        self.ùúÜ = cfg.ùúÜ\n",
    "        self.p = cfg.p\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"generate data sequences over the specified number of seeds\"\"\"\n",
    "        xseq, yseq, tseq, taskseq = [], [], [], []\n",
    "        for _ in range(self.num_seeds):\n",
    "            dat = self.generate_sequence(np.random.randint(0, 10000))\n",
    "            xseq.append(dat[0])\n",
    "            yseq.append(dat[1])\n",
    "            tseq.append(dat[2])\n",
    "            taskseq.append(dat[3])\n",
    "\n",
    "        self.data = {'x': xseq,\n",
    "                     'y': yseq,\n",
    "                     't': tseq,\n",
    "                     'task': taskseq}\n",
    "\n",
    "    def generate_sequence(self, seed):\n",
    "        \"\"\"generate a sequence of data\"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        T = self.period\n",
    "\n",
    "        n_list = np.random.poisson(self.ùúÜ, self.seq_len)\n",
    "        drop_list = np.random.binomial(1, self.p, self.seq_len).astype(bool)\n",
    "\n",
    "        data = []\n",
    "        taskseq = []\n",
    "        for t, (n, drop) in enumerate(zip(n_list, drop_list)):\n",
    "            current_task = (t % T) < (T // 2)\n",
    "            taskseq.append(int(current_task))\n",
    "\n",
    "            if drop:\n",
    "                continue\n",
    "\n",
    "            x1 = np.random.uniform(-2, -1, n)\n",
    "            x2 = np.random.uniform(1, 2, n)\n",
    "            mask = np.random.choice([0, 1], p=[0.5, 0.5], size=n)\n",
    "            x = x1 * mask + x2 * (1 - mask)\n",
    "\n",
    "            if current_task:\n",
    "                y = x > 0\n",
    "            else:\n",
    "                y = x < 0      \n",
    "        \n",
    "            data.append([x, y, t * np.ones(n)])\n",
    "        \n",
    "        data = np.concatenate(data, axis=-1)\n",
    "        Xdat = data[0, :].reshape(-1, 1)\n",
    "        Ydat = data[1, :].astype(int)\n",
    "        tind = data[2, :]\n",
    "        return Xdat, Ydat, tind, taskseq\n",
    "    \n",
    "    def generate_at_time(self, t, num_samples):\n",
    "        \"\"\"generate a test sample of data (x, y) ~ p_t from the marginal \n",
    "        of the process at time t. This is used to evaluate the instantaneous\n",
    "        loss of the predictor\n",
    "\n",
    "        might not be needed\n",
    "        \"\"\"\n",
    "        # Generate samples from U[-2, -1] union U[1, 2]\n",
    "        x1 = np.random.uniform(-2, -1, num_samples)\n",
    "        x2 = np.random.uniform(1, 2, num_samples)\n",
    "        mask = np.random.choice([0, 1], p=[0.5, 0.5], size=num_samples)\n",
    "        Xdat = x1 * mask + x2 * (1 - mask)\n",
    "\n",
    "        # create labels\n",
    "        T = self.period\n",
    "        if (t % T) < (T // 2):\n",
    "            Ydat = Xdat < 0\n",
    "        else:\n",
    "            Ydat = Xdat > 0\n",
    "\n",
    "        Xdat = Xdat.reshape(-1, 1)\n",
    "        tdat = t * np.ones(num_samples)\n",
    "\n",
    "        x = torch.from_numpy(Xdat).float()\n",
    "        y = torch.from_numpy(Ydat).long()\n",
    "        t = torch.from_numpy(tdat).float()\n",
    "        return x, y, t\n",
    "    \n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"Form the torch dataset\"\"\"\n",
    "    def __init__(self, data, present, run_id, test, past=None):\n",
    "        self.x = torch.from_numpy(data['x'][run_id]).float()\n",
    "        self.y = torch.from_numpy(data['y'][run_id]).long()\n",
    "        self.t = torch.from_numpy(data['t'][run_id]).float()\n",
    "\n",
    "        if test:\n",
    "            # Use data from time 'present' onwards for testing\n",
    "            test_idx = torch.where(self.t >= present)\n",
    "            self.x = self.x[test_idx]\n",
    "            self.y = self.y[test_idx]\n",
    "            self.t = self.t[test_idx]\n",
    "        else:\n",
    "            if past is None:\n",
    "                #  # Use data up to time 'idx' onwards for training (full history)\n",
    "                train_idx = torch.where(self.t < present)\n",
    "                self.x = self.x[train_idx]\n",
    "                self.y = self.y[train_idx]\n",
    "                self.t = self.t[train_idx]\n",
    "            else:\n",
    "                # Use the most recent past data up to time 'idx' onwards for training (partial history)\n",
    "                train_idx = torch.where((self.t >= present-past) & (self.t < present))\n",
    "                self.x = self.x[train_idx]\n",
    "                self.y = self.y[train_idx]\n",
    "                self.t = self.t[train_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        t = self.t[idx]\n",
    "        return x, y, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 40\n",
    "seq_len = 1000\n",
    "cfg = ProcessConfig(\n",
    "    period=period,\n",
    "    seq_len=seq_len,\n",
    "    num_seeds=3\n",
    ")\n",
    "dp = DataGeneratingProcess(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, t, task = dp.generate_sequence(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1000 artists>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "sample_count_per_time = [sum(t == i) for i in range(0, seq_len)]\n",
    "colors = ['blue' if tsk == 1 else 'red' for tsk in task]\n",
    "ax.bar(np.arange(0, 1000), sample_count_per_time, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"form the time-embedding\"\"\"\n",
    "    def __init__(self, tdim=10):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.freqs = (2 * np.pi) / torch.arange(2, tdim + 1, 2).unsqueeze(0)\n",
    "\n",
    "    def forward(self, t):\n",
    "        sin_emb = torch.sin(self.freqs.to(t.device) * t)\n",
    "        cos_emb = torch.cos(self.freqs.to(t.device) * t)\n",
    "        return torch.cat([sin_emb, cos_emb], dim=-1)\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"a simple MLP architecture for implementing both vanilla-MLP and\n",
    "    prospective-MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=1, out_dim=2, hidden_dim=32, tdim=50, prospective=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.prospective = prospective\n",
    "        if prospective:\n",
    "            self.time_embed = TimeEmbedding(tdim=tdim)\n",
    "            self.fc1 = nn.Linear(in_dim + tdim, hidden_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.prospective:\n",
    "            t = self.time_embed(t.unsqueeze(-1))\n",
    "            x = torch.cat([x, t], dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_nets.utils.bgd import BGD\n",
    "\n",
    "def set_seed(acorn):\n",
    "    \"\"\"set random seed\"\"\"\n",
    "    random.seed(acorn)\n",
    "    np.random.seed(acorn)\n",
    "    torch.manual_seed(acorn)\n",
    "    torch.cuda.manual_seed(acorn)\n",
    "    torch.cuda.manual_seed_all(acorn)\n",
    "\n",
    "def get_dataloaders(dp, t, seed, past=None):\n",
    "    \"\"\"obtain the dataloaders\"\"\"\n",
    "    # TODO: Recheck data loaders before running OGD\n",
    "    if past:\n",
    "        if t < 16:\n",
    "            train_dataset = SyntheticDataset(dp.data, t, seed, test=False, past=None)\n",
    "        else:\n",
    "            train_dataset = SyntheticDataset(dp.data, t, seed, test=False, past=past)\n",
    "    else:\n",
    "        train_dataset = SyntheticDataset(dp.data, t, seed, test=False, past=None)\n",
    "    test_dataset = SyntheticDataset(dp.data, t, seed, test=True)\n",
    "    ttest_dataset = TensorDataset(*dp.generate_at_time(t, 500))\n",
    "\n",
    "    if t < 32:\n",
    "        bs = 2\n",
    "    else:\n",
    "        if past:\n",
    "            if t < 8: bs = 2\n",
    "            else: bs = 8\n",
    "        else:\n",
    "            bs = 32\n",
    "            \n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "    ttestloader = DataLoader(\n",
    "        ttest_dataset, \n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return trainloader, testloader, ttestloader\n",
    "\n",
    "def get_optimizer(model, bgd=False):\n",
    "    \"\"\"Obtain the appropriate optimizer\"\"\"\n",
    "    if bgd:\n",
    "        params = [{'params': [p]} for p in model.parameters()]\n",
    "        optimizer = BGD(params, std_init=0.01)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=0.1,\n",
    "            momentum=0.9, \n",
    "            nesterov=True,\n",
    "            weight_decay=0.00001\n",
    "        )\n",
    "    return optimizer\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"trainer class for the models\"\"\"\n",
    "    def __init__(self, model, train_loader, test_loader, ttest_loader, optimizer, device, verbose=False):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.ttest_loader = ttest_loader\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.bgd = True if optimizer.__class__.__name__ == 'BGD' else False\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            batch = [b.to(self.device) for b in batch]\n",
    "            x, y, t = batch\n",
    "\n",
    "            if self.bgd:\n",
    "                bs = x.size(0)\n",
    "                for mc_iter in range(10):\n",
    "                    self.optimizer.randomize_weights()\n",
    "                    outputs = self.model(x, t)\n",
    "                    loss = self.criterion(outputs.squeeze(), y)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.aggregate_grads(bs)\n",
    "            else:\n",
    "                outputs = self.model(x, t)\n",
    "                loss = self.criterion(outputs.squeeze(), y)\n",
    "                self.optimizer.zero_grad()  \n",
    "                loss.backward()  \n",
    "\n",
    "            self.optimizer.step() \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def evaluate(self, model=None):\n",
    "        if model:\n",
    "            self.model = model\n",
    "        self.model.eval()\n",
    "        perrs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_loader:\n",
    "                batch = [b.to(self.device) for b in batch]\n",
    "                x, y, t = batch\n",
    "                logits = self.model(x, t)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                err = (probs.argmax(dim=1) != y).float()\n",
    "                perrs.append(err.cpu().numpy())\n",
    "        perrs = np.concatenate(perrs)\n",
    "        ploss = perrs.mean()\n",
    "\n",
    "        errs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.ttest_loader:\n",
    "                batch = [b.to(self.device) for b in batch]\n",
    "                x, y, t = batch\n",
    "                logits = self.model(x, t)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                err = (probs.argmax(dim=1) != y).float()\n",
    "                errs.append(err.cpu().numpy())\n",
    "        iloss = np.concatenate(errs).mean()\n",
    "        return iloss, ploss, perrs\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            if self.verbose:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                    f'Train Loss: {train_loss:.4f}, ')\n",
    "        print(f\"training loss at last epoch: {train_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 0\n",
      "time t = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwindesilva/miniforge3/envs/prol/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at last epoch: 0.000\n",
      "time t = 21\n",
      "training loss at last epoch: 0.709\n",
      "time t = 22\n",
      "training loss at last epoch: 0.733\n",
      "time t = 23\n",
      "training loss at last epoch: 0.704\n",
      "time t = 24\n",
      "training loss at last epoch: 0.768\n",
      "time t = 25\n",
      "training loss at last epoch: 0.711\n",
      "time t = 26\n",
      "training loss at last epoch: 0.746\n",
      "time t = 27\n",
      "training loss at last epoch: 0.733\n",
      "time t = 28\n",
      "training loss at last epoch: 0.722\n",
      "time t = 29\n",
      "training loss at last epoch: 0.761\n",
      "time t = 30\n",
      "training loss at last epoch: 0.741\n",
      "time t = 31\n",
      "training loss at last epoch: 0.718\n",
      "time t = 32\n",
      "training loss at last epoch: 0.662\n",
      "time t = 33\n",
      "training loss at last epoch: 0.684\n",
      "time t = 34\n",
      "training loss at last epoch: 0.679\n",
      "time t = 35\n",
      "training loss at last epoch: 0.680\n",
      "time t = 36\n",
      "training loss at last epoch: 0.687\n",
      "time t = 37\n",
      "training loss at last epoch: 0.683\n",
      "time t = 38\n",
      "training loss at last epoch: 0.687\n",
      "time t = 39\n",
      "training loss at last epoch: 0.681\n",
      "time t = 40\n",
      "training loss at last epoch: 0.681\n",
      "time t = 41\n",
      "training loss at last epoch: 0.687\n",
      "time t = 42\n",
      "training loss at last epoch: 0.690\n",
      "time t = 43\n",
      "training loss at last epoch: 0.690\n",
      "time t = 44\n",
      "training loss at last epoch: 0.700\n",
      "time t = 45\n",
      "training loss at last epoch: 0.692\n",
      "time t = 46\n",
      "training loss at last epoch: 0.683\n",
      "time t = 47\n",
      "training loss at last epoch: 0.686\n",
      "time t = 48\n",
      "training loss at last epoch: 0.681\n",
      "time t = 49\n",
      "training loss at last epoch: 0.683\n",
      "time t = 50\n",
      "training loss at last epoch: 0.682\n",
      "time t = 51\n",
      "training loss at last epoch: 0.669\n",
      "time t = 52\n",
      "training loss at last epoch: 0.669\n",
      "time t = 53\n",
      "training loss at last epoch: 0.672\n",
      "time t = 54\n",
      "training loss at last epoch: 0.663\n",
      "time t = 55\n",
      "training loss at last epoch: 0.666\n",
      "time t = 56\n",
      "training loss at last epoch: 0.644\n",
      "time t = 57\n",
      "training loss at last epoch: 0.650\n",
      "time t = 58\n",
      "training loss at last epoch: 0.643\n",
      "time t = 59\n",
      "training loss at last epoch: 0.645\n",
      "time t = 60\n",
      "training loss at last epoch: 0.627\n",
      "time t = 61\n",
      "training loss at last epoch: 0.649\n",
      "time t = 62\n",
      "training loss at last epoch: 0.645\n",
      "time t = 63\n",
      "training loss at last epoch: 0.670\n",
      "time t = 64\n",
      "training loss at last epoch: 0.667\n",
      "time t = 65\n",
      "training loss at last epoch: 0.674\n",
      "time t = 66\n",
      "training loss at last epoch: 0.678\n",
      "time t = 67\n",
      "training loss at last epoch: 0.681\n",
      "time t = 68\n",
      "training loss at last epoch: 0.702\n",
      "time t = 69\n",
      "training loss at last epoch: 0.683\n",
      "time t = 70\n",
      "training loss at last epoch: 0.678\n",
      "time t = 71\n",
      "training loss at last epoch: 0.686\n",
      "time t = 72\n",
      "training loss at last epoch: 0.688\n",
      "time t = 73\n",
      "training loss at last epoch: 0.686\n",
      "time t = 74\n",
      "training loss at last epoch: 0.687\n",
      "time t = 75\n",
      "training loss at last epoch: 0.688\n",
      "time t = 76\n",
      "training loss at last epoch: 0.694\n",
      "time t = 77\n",
      "training loss at last epoch: 0.694\n",
      "time t = 78\n",
      "training loss at last epoch: 0.694\n",
      "time t = 79\n",
      "training loss at last epoch: 0.690\n",
      "time t = 80\n",
      "training loss at last epoch: 0.691\n",
      "time t = 81\n",
      "training loss at last epoch: 0.688\n",
      "time t = 82\n",
      "training loss at last epoch: 0.694\n",
      "time t = 83\n",
      "training loss at last epoch: 0.689\n",
      "time t = 84\n",
      "training loss at last epoch: 0.689\n",
      "time t = 85\n",
      "training loss at last epoch: 0.689\n",
      "time t = 86\n",
      "training loss at last epoch: 0.694\n",
      "time t = 87\n",
      "training loss at last epoch: 0.690\n",
      "time t = 88\n",
      "training loss at last epoch: 0.691\n",
      "time t = 89\n",
      "training loss at last epoch: 0.686\n",
      "time t = 90\n",
      "training loss at last epoch: 0.686\n",
      "time t = 91\n",
      "training loss at last epoch: 0.684\n",
      "time t = 92\n",
      "training loss at last epoch: 0.679\n",
      "time t = 93\n",
      "training loss at last epoch: 0.687\n",
      "time t = 94\n",
      "training loss at last epoch: 0.683\n",
      "time t = 95\n",
      "training loss at last epoch: 0.686\n",
      "time t = 96\n",
      "training loss at last epoch: 0.693\n",
      "time t = 97\n",
      "training loss at last epoch: 0.697\n",
      "time t = 98\n",
      "training loss at last epoch: 0.684\n",
      "time t = 99\n",
      "training loss at last epoch: 0.681\n",
      "time t = 100\n",
      "training loss at last epoch: 0.686\n",
      "time t = 101\n",
      "training loss at last epoch: 0.682\n",
      "time t = 102\n",
      "training loss at last epoch: 0.686\n",
      "time t = 103\n",
      "training loss at last epoch: 0.687\n",
      "time t = 104\n",
      "training loss at last epoch: 0.682\n",
      "time t = 105\n",
      "training loss at last epoch: 0.692\n",
      "time t = 106\n",
      "training loss at last epoch: 0.691\n",
      "time t = 107\n",
      "training loss at last epoch: 0.690\n",
      "time t = 108\n",
      "training loss at last epoch: 0.690\n",
      "time t = 109\n",
      "training loss at last epoch: 0.694\n",
      "time t = 110\n",
      "training loss at last epoch: 0.696\n",
      "time t = 111\n",
      "training loss at last epoch: 0.694\n",
      "time t = 112\n",
      "training loss at last epoch: 0.696\n",
      "time t = 113\n",
      "training loss at last epoch: 0.695\n",
      "time t = 114\n",
      "training loss at last epoch: 0.692\n",
      "time t = 115\n",
      "training loss at last epoch: 0.697\n",
      "time t = 116\n",
      "training loss at last epoch: 0.696\n",
      "time t = 117\n",
      "training loss at last epoch: 0.694\n",
      "time t = 118\n",
      "training loss at last epoch: 0.695\n",
      "time t = 119\n",
      "training loss at last epoch: 0.695\n",
      "time t = 120\n",
      "training loss at last epoch: 0.697\n",
      "time t = 121\n",
      "training loss at last epoch: 0.696\n",
      "time t = 122\n",
      "training loss at last epoch: 0.698\n",
      "time t = 123\n",
      "training loss at last epoch: 0.701\n",
      "time t = 124\n",
      "training loss at last epoch: 0.696\n",
      "time t = 125\n",
      "training loss at last epoch: 0.695\n",
      "time t = 126\n",
      "training loss at last epoch: 0.695\n",
      "time t = 127\n",
      "training loss at last epoch: 0.695\n",
      "time t = 128\n",
      "training loss at last epoch: 0.694\n",
      "time t = 129\n",
      "training loss at last epoch: 0.694\n",
      "time t = 130\n",
      "training loss at last epoch: 0.693\n",
      "time t = 131\n",
      "training loss at last epoch: 0.694\n",
      "time t = 132\n",
      "training loss at last epoch: 0.693\n",
      "time t = 133\n",
      "training loss at last epoch: 0.691\n",
      "time t = 134\n",
      "training loss at last epoch: 0.693\n",
      "time t = 135\n",
      "training loss at last epoch: 0.703\n",
      "time t = 136\n",
      "training loss at last epoch: 0.694\n",
      "time t = 137\n",
      "training loss at last epoch: 0.692\n",
      "time t = 138\n",
      "training loss at last epoch: 0.694\n",
      "time t = 139\n",
      "training loss at last epoch: 0.705\n",
      "time t = 140\n",
      "training loss at last epoch: 0.690\n",
      "time t = 141\n",
      "training loss at last epoch: 0.688\n",
      "time t = 142\n",
      "training loss at last epoch: 0.696\n",
      "time t = 143\n",
      "training loss at last epoch: 0.691\n",
      "time t = 144\n",
      "training loss at last epoch: 0.692\n",
      "time t = 145\n",
      "training loss at last epoch: 0.692\n",
      "time t = 146\n",
      "training loss at last epoch: 0.693\n",
      "time t = 147\n",
      "training loss at last epoch: 0.699\n",
      "time t = 148\n",
      "training loss at last epoch: 0.697\n",
      "time t = 149\n",
      "training loss at last epoch: 0.695\n",
      "time t = 150\n",
      "training loss at last epoch: 0.699\n",
      "time t = 151\n",
      "training loss at last epoch: 0.698\n",
      "time t = 152\n",
      "training loss at last epoch: 0.701\n",
      "time t = 153\n",
      "training loss at last epoch: 0.695\n",
      "time t = 154\n",
      "training loss at last epoch: 0.697\n",
      "time t = 155\n",
      "training loss at last epoch: 0.692\n",
      "time t = 156\n",
      "training loss at last epoch: 0.699\n",
      "time t = 157\n",
      "training loss at last epoch: 0.694\n",
      "time t = 158\n",
      "training loss at last epoch: 0.696\n",
      "time t = 159\n",
      "training loss at last epoch: 0.695\n",
      "time t = 160\n",
      "training loss at last epoch: 0.697\n",
      "time t = 161\n",
      "training loss at last epoch: 0.693\n",
      "time t = 162\n",
      "training loss at last epoch: 0.693\n",
      "time t = 163\n",
      "training loss at last epoch: 0.696\n",
      "time t = 164\n",
      "training loss at last epoch: 0.692\n",
      "time t = 165\n",
      "training loss at last epoch: 0.698\n",
      "time t = 166\n",
      "training loss at last epoch: 0.695\n",
      "time t = 167\n",
      "training loss at last epoch: 0.695\n",
      "time t = 168\n",
      "training loss at last epoch: 0.694\n",
      "time t = 169\n",
      "training loss at last epoch: 0.696\n",
      "time t = 170\n",
      "training loss at last epoch: 0.694\n",
      "time t = 171\n",
      "training loss at last epoch: 0.696\n",
      "time t = 172\n",
      "training loss at last epoch: 0.694\n",
      "time t = 173\n",
      "training loss at last epoch: 0.698\n",
      "time t = 174\n",
      "training loss at last epoch: 0.692\n",
      "time t = 175\n",
      "training loss at last epoch: 0.693\n",
      "time t = 176\n",
      "training loss at last epoch: 0.693\n",
      "time t = 177\n",
      "training loss at last epoch: 0.692\n",
      "time t = 178\n",
      "training loss at last epoch: 0.697\n",
      "time t = 179\n",
      "training loss at last epoch: 0.695\n",
      "time t = 180\n",
      "training loss at last epoch: 0.689\n",
      "time t = 181\n",
      "training loss at last epoch: 0.691\n",
      "time t = 182\n",
      "training loss at last epoch: 0.689\n",
      "time t = 183\n",
      "training loss at last epoch: 0.690\n",
      "time t = 184\n",
      "training loss at last epoch: 0.693\n",
      "time t = 185\n",
      "training loss at last epoch: 0.695\n",
      "time t = 186\n",
      "training loss at last epoch: 0.696\n",
      "time t = 187\n",
      "training loss at last epoch: 0.695\n",
      "time t = 188\n",
      "training loss at last epoch: 0.694\n",
      "time t = 189\n",
      "training loss at last epoch: 0.693\n",
      "time t = 190\n",
      "training loss at last epoch: 0.697\n",
      "time t = 191\n",
      "training loss at last epoch: 0.703\n",
      "time t = 192\n",
      "training loss at last epoch: 0.694\n",
      "time t = 193\n",
      "training loss at last epoch: 0.697\n",
      "time t = 194\n",
      "training loss at last epoch: 0.697\n",
      "time t = 195\n",
      "training loss at last epoch: 0.694\n",
      "time t = 196\n",
      "training loss at last epoch: 0.695\n",
      "time t = 197\n",
      "training loss at last epoch: 0.696\n",
      "time t = 198\n",
      "training loss at last epoch: 0.696\n",
      "time t = 199\n",
      "training loss at last epoch: 0.695\n",
      "time t = 200\n",
      "training loss at last epoch: 0.696\n",
      "time t = 201\n",
      "training loss at last epoch: 0.696\n",
      "time t = 202\n",
      "training loss at last epoch: 0.698\n",
      "time t = 203\n",
      "training loss at last epoch: 0.697\n",
      "time t = 204\n",
      "training loss at last epoch: 0.696\n",
      "time t = 205\n",
      "training loss at last epoch: 0.697\n",
      "time t = 206\n",
      "training loss at last epoch: 0.694\n",
      "time t = 207\n",
      "training loss at last epoch: 0.696\n",
      "time t = 208\n",
      "training loss at last epoch: 0.694\n",
      "time t = 209\n",
      "training loss at last epoch: 0.696\n",
      "time t = 210\n",
      "training loss at last epoch: 0.697\n",
      "time t = 211\n",
      "training loss at last epoch: 0.695\n",
      "time t = 212\n",
      "training loss at last epoch: 0.697\n",
      "time t = 213\n",
      "training loss at last epoch: 0.694\n",
      "time t = 214\n",
      "training loss at last epoch: 0.695\n",
      "time t = 215\n",
      "training loss at last epoch: 0.696\n",
      "time t = 216\n",
      "training loss at last epoch: 0.697\n",
      "time t = 217\n",
      "training loss at last epoch: 0.698\n",
      "time t = 218\n",
      "training loss at last epoch: 0.694\n",
      "time t = 219\n",
      "training loss at last epoch: 0.692\n",
      "time t = 220\n",
      "training loss at last epoch: 0.694\n",
      "time t = 221\n",
      "training loss at last epoch: 0.692\n",
      "time t = 222\n",
      "training loss at last epoch: 0.695\n",
      "time t = 223\n",
      "training loss at last epoch: 0.696\n",
      "time t = 224\n",
      "training loss at last epoch: 0.697\n",
      "time t = 225\n",
      "training loss at last epoch: 0.692\n",
      "time t = 226\n",
      "training loss at last epoch: 0.694\n",
      "time t = 227\n",
      "training loss at last epoch: 0.696\n",
      "time t = 228\n",
      "training loss at last epoch: 0.695\n",
      "time t = 229\n",
      "training loss at last epoch: 0.694\n",
      "time t = 230\n",
      "training loss at last epoch: 0.692\n",
      "time t = 231\n",
      "training loss at last epoch: 0.697\n",
      "time t = 232\n",
      "training loss at last epoch: 0.695\n",
      "time t = 233\n",
      "training loss at last epoch: 0.695\n",
      "time t = 234\n",
      "training loss at last epoch: 0.695\n",
      "time t = 235\n",
      "training loss at last epoch: 0.698\n",
      "time t = 236\n",
      "training loss at last epoch: 0.696\n",
      "time t = 237\n",
      "training loss at last epoch: 0.695\n",
      "time t = 238\n",
      "training loss at last epoch: 0.696\n",
      "time t = 239\n",
      "training loss at last epoch: 0.697\n",
      "time t = 240\n",
      "training loss at last epoch: 0.694\n",
      "time t = 241\n",
      "training loss at last epoch: 0.696\n",
      "time t = 242\n",
      "training loss at last epoch: 0.697\n",
      "time t = 243\n",
      "training loss at last epoch: 0.697\n",
      "time t = 244\n",
      "training loss at last epoch: 0.697\n",
      "time t = 245\n",
      "training loss at last epoch: 0.691\n",
      "time t = 246\n",
      "training loss at last epoch: 0.694\n",
      "time t = 247\n",
      "training loss at last epoch: 0.695\n",
      "time t = 248\n",
      "training loss at last epoch: 0.697\n",
      "time t = 249\n",
      "training loss at last epoch: 0.696\n",
      "time t = 250\n",
      "training loss at last epoch: 0.695\n",
      "time t = 251\n",
      "training loss at last epoch: 0.694\n",
      "time t = 252\n",
      "training loss at last epoch: 0.695\n",
      "time t = 253\n",
      "training loss at last epoch: 0.693\n",
      "time t = 254\n",
      "training loss at last epoch: 0.695\n",
      "time t = 255\n",
      "training loss at last epoch: 0.694\n",
      "time t = 256\n",
      "training loss at last epoch: 0.695\n",
      "time t = 257\n",
      "training loss at last epoch: 0.695\n",
      "time t = 258\n",
      "training loss at last epoch: 0.695\n",
      "time t = 259\n",
      "training loss at last epoch: 0.695\n",
      "time t = 260\n",
      "training loss at last epoch: 0.694\n",
      "time t = 261\n",
      "training loss at last epoch: 0.695\n",
      "time t = 262\n",
      "training loss at last epoch: 0.694\n",
      "time t = 263\n",
      "training loss at last epoch: 0.694\n",
      "time t = 264\n",
      "training loss at last epoch: 0.698\n",
      "time t = 265\n",
      "training loss at last epoch: 0.696\n",
      "time t = 266\n",
      "training loss at last epoch: 0.697\n",
      "time t = 267\n",
      "training loss at last epoch: 0.696\n",
      "time t = 268\n",
      "training loss at last epoch: 0.695\n",
      "time t = 269\n",
      "training loss at last epoch: 0.696\n",
      "time t = 270\n",
      "training loss at last epoch: 0.696\n",
      "time t = 271\n",
      "training loss at last epoch: 0.693\n",
      "time t = 272\n",
      "training loss at last epoch: 0.698\n",
      "time t = 273\n",
      "training loss at last epoch: 0.694\n",
      "time t = 274\n",
      "training loss at last epoch: 0.700\n",
      "time t = 275\n",
      "training loss at last epoch: 0.696\n",
      "time t = 276\n",
      "training loss at last epoch: 0.695\n",
      "time t = 277\n",
      "training loss at last epoch: 0.695\n",
      "time t = 278\n",
      "training loss at last epoch: 0.697\n",
      "time t = 279\n",
      "training loss at last epoch: 0.697\n",
      "time t = 280\n",
      "training loss at last epoch: 0.695\n",
      "time t = 281\n",
      "training loss at last epoch: 0.695\n",
      "time t = 282\n",
      "training loss at last epoch: 0.697\n",
      "time t = 283\n",
      "training loss at last epoch: 0.695\n",
      "time t = 284\n",
      "training loss at last epoch: 0.695\n",
      "time t = 285\n",
      "training loss at last epoch: 0.695\n",
      "time t = 286\n",
      "training loss at last epoch: 0.695\n",
      "time t = 287\n",
      "training loss at last epoch: 0.696\n",
      "time t = 288\n",
      "training loss at last epoch: 0.694\n",
      "time t = 289\n",
      "training loss at last epoch: 0.694\n",
      "time t = 290\n",
      "training loss at last epoch: 0.696\n",
      "time t = 291\n",
      "training loss at last epoch: 0.695\n",
      "time t = 292\n",
      "training loss at last epoch: 0.695\n",
      "time t = 293\n",
      "training loss at last epoch: 0.696\n",
      "time t = 294\n",
      "training loss at last epoch: 0.694\n",
      "time t = 295\n",
      "training loss at last epoch: 0.695\n",
      "time t = 296\n",
      "training loss at last epoch: 0.697\n",
      "time t = 297\n",
      "training loss at last epoch: 0.694\n",
      "time t = 298\n",
      "training loss at last epoch: 0.693\n",
      "time t = 299\n",
      "training loss at last epoch: 0.696\n",
      "time t = 300\n",
      "training loss at last epoch: 0.694\n",
      "time t = 301\n",
      "training loss at last epoch: 0.695\n",
      "time t = 302\n",
      "training loss at last epoch: 0.695\n",
      "time t = 303\n",
      "training loss at last epoch: 0.694\n",
      "time t = 304\n",
      "training loss at last epoch: 0.696\n",
      "time t = 305\n",
      "training loss at last epoch: 0.695\n",
      "time t = 306\n",
      "training loss at last epoch: 0.697\n",
      "time t = 307\n",
      "training loss at last epoch: 0.693\n",
      "time t = 308\n",
      "training loss at last epoch: 0.697\n",
      "time t = 309\n",
      "training loss at last epoch: 0.698\n",
      "time t = 310\n",
      "training loss at last epoch: 0.697\n",
      "time t = 311\n",
      "training loss at last epoch: 0.695\n",
      "time t = 312\n",
      "training loss at last epoch: 0.695\n",
      "time t = 313\n",
      "training loss at last epoch: 0.696\n",
      "time t = 314\n",
      "training loss at last epoch: 0.696\n",
      "time t = 315\n",
      "training loss at last epoch: 0.694\n",
      "time t = 316\n",
      "training loss at last epoch: 0.695\n",
      "time t = 317\n",
      "training loss at last epoch: 0.695\n",
      "time t = 318\n",
      "training loss at last epoch: 0.695\n",
      "time t = 319\n",
      "training loss at last epoch: 0.697\n",
      "time t = 320\n",
      "training loss at last epoch: 0.696\n",
      "time t = 321\n",
      "training loss at last epoch: 0.695\n",
      "time t = 322\n",
      "training loss at last epoch: 0.696\n",
      "time t = 323\n",
      "training loss at last epoch: 0.698\n",
      "time t = 324\n",
      "training loss at last epoch: 0.696\n",
      "time t = 325\n",
      "training loss at last epoch: 0.700\n",
      "time t = 326\n",
      "training loss at last epoch: 0.697\n",
      "time t = 327\n",
      "training loss at last epoch: 0.696\n",
      "time t = 328\n",
      "training loss at last epoch: 0.694\n",
      "time t = 329\n",
      "training loss at last epoch: 0.697\n",
      "time t = 330\n",
      "training loss at last epoch: 0.696\n",
      "time t = 331\n",
      "training loss at last epoch: 0.693\n",
      "time t = 332\n",
      "training loss at last epoch: 0.695\n",
      "time t = 333\n",
      "training loss at last epoch: 0.695\n",
      "time t = 334\n",
      "training loss at last epoch: 0.696\n",
      "time t = 335\n",
      "training loss at last epoch: 0.696\n",
      "time t = 336\n",
      "training loss at last epoch: 0.695\n",
      "time t = 337\n",
      "training loss at last epoch: 0.695\n",
      "time t = 338\n",
      "training loss at last epoch: 0.696\n",
      "time t = 339\n",
      "training loss at last epoch: 0.696\n",
      "time t = 340\n",
      "training loss at last epoch: 0.694\n",
      "time t = 341\n",
      "training loss at last epoch: 0.694\n",
      "time t = 342\n",
      "training loss at last epoch: 0.696\n",
      "time t = 343\n",
      "training loss at last epoch: 0.694\n",
      "time t = 344\n",
      "training loss at last epoch: 0.697\n",
      "time t = 345\n",
      "training loss at last epoch: 0.696\n",
      "time t = 346\n",
      "training loss at last epoch: 0.697\n",
      "time t = 347\n",
      "training loss at last epoch: 0.696\n",
      "time t = 348\n",
      "training loss at last epoch: 0.697\n",
      "time t = 349\n",
      "training loss at last epoch: 0.694\n",
      "time t = 350\n",
      "training loss at last epoch: 0.697\n",
      "time t = 351\n",
      "training loss at last epoch: 0.693\n",
      "time t = 352\n",
      "training loss at last epoch: 0.696\n",
      "time t = 353\n",
      "training loss at last epoch: 0.695\n",
      "time t = 354\n",
      "training loss at last epoch: 0.698\n",
      "time t = 355\n",
      "training loss at last epoch: 0.694\n",
      "time t = 356\n",
      "training loss at last epoch: 0.694\n",
      "time t = 357\n",
      "training loss at last epoch: 0.694\n",
      "time t = 358\n",
      "training loss at last epoch: 0.697\n",
      "time t = 359\n",
      "training loss at last epoch: 0.697\n",
      "time t = 360\n",
      "training loss at last epoch: 0.696\n",
      "time t = 361\n",
      "training loss at last epoch: 0.696\n",
      "time t = 362\n",
      "training loss at last epoch: 0.695\n",
      "time t = 363\n",
      "training loss at last epoch: 0.694\n",
      "time t = 364\n",
      "training loss at last epoch: 0.696\n",
      "time t = 365\n",
      "training loss at last epoch: 0.695\n",
      "time t = 366\n",
      "training loss at last epoch: 0.697\n",
      "time t = 367\n",
      "training loss at last epoch: 0.696\n",
      "time t = 368\n",
      "training loss at last epoch: 0.696\n",
      "time t = 369\n",
      "training loss at last epoch: 0.695\n",
      "time t = 370\n",
      "training loss at last epoch: 0.695\n",
      "time t = 371\n",
      "training loss at last epoch: 0.695\n",
      "time t = 372\n",
      "training loss at last epoch: 0.695\n",
      "time t = 373\n",
      "training loss at last epoch: 0.696\n",
      "time t = 374\n",
      "training loss at last epoch: 0.695\n",
      "time t = 375\n",
      "training loss at last epoch: 0.698\n",
      "time t = 376\n",
      "training loss at last epoch: 0.696\n",
      "time t = 377\n",
      "training loss at last epoch: 0.697\n",
      "time t = 378\n",
      "training loss at last epoch: 0.695\n",
      "time t = 379\n",
      "training loss at last epoch: 0.694\n",
      "time t = 380\n",
      "training loss at last epoch: 0.696\n",
      "time t = 381\n",
      "training loss at last epoch: 0.695\n",
      "time t = 382\n",
      "training loss at last epoch: 0.693\n",
      "time t = 383\n",
      "training loss at last epoch: 0.696\n",
      "time t = 384\n",
      "training loss at last epoch: 0.697\n",
      "time t = 385\n",
      "training loss at last epoch: 0.697\n",
      "time t = 386\n",
      "training loss at last epoch: 0.695\n",
      "time t = 387\n",
      "training loss at last epoch: 0.696\n",
      "time t = 388\n",
      "training loss at last epoch: 0.697\n",
      "time t = 389\n",
      "training loss at last epoch: 0.695\n",
      "time t = 390\n",
      "training loss at last epoch: 0.696\n",
      "time t = 391\n",
      "training loss at last epoch: 0.695\n",
      "time t = 392\n",
      "training loss at last epoch: 0.696\n",
      "time t = 393\n",
      "training loss at last epoch: 0.695\n",
      "time t = 394\n",
      "training loss at last epoch: 0.695\n",
      "time t = 395\n",
      "training loss at last epoch: 0.695\n",
      "time t = 396\n",
      "training loss at last epoch: 0.696\n",
      "time t = 397\n",
      "training loss at last epoch: 0.696\n",
      "time t = 398\n",
      "training loss at last epoch: 0.694\n",
      "time t = 399\n",
      "training loss at last epoch: 0.695\n",
      "time t = 400\n",
      "training loss at last epoch: 0.696\n",
      "time t = 401\n",
      "training loss at last epoch: 0.695\n",
      "time t = 402\n",
      "training loss at last epoch: 0.698\n",
      "time t = 403\n",
      "training loss at last epoch: 0.695\n",
      "time t = 404\n",
      "training loss at last epoch: 0.698\n",
      "time t = 405\n",
      "training loss at last epoch: 0.694\n",
      "time t = 406\n",
      "training loss at last epoch: 0.697\n",
      "time t = 407\n",
      "training loss at last epoch: 0.696\n",
      "time t = 408\n",
      "training loss at last epoch: 0.694\n",
      "time t = 409\n",
      "training loss at last epoch: 0.696\n",
      "time t = 410\n",
      "training loss at last epoch: 0.695\n",
      "time t = 411\n",
      "training loss at last epoch: 0.696\n",
      "time t = 412\n",
      "training loss at last epoch: 0.697\n",
      "time t = 413\n",
      "training loss at last epoch: 0.696\n",
      "time t = 414\n",
      "training loss at last epoch: 0.697\n",
      "time t = 415\n",
      "training loss at last epoch: 0.695\n",
      "time t = 416\n",
      "training loss at last epoch: 0.696\n",
      "time t = 417\n",
      "training loss at last epoch: 0.696\n",
      "time t = 418\n",
      "training loss at last epoch: 0.694\n",
      "time t = 419\n",
      "training loss at last epoch: 0.696\n",
      "time t = 420\n",
      "training loss at last epoch: 0.696\n",
      "time t = 421\n",
      "training loss at last epoch: 0.696\n",
      "time t = 422\n",
      "training loss at last epoch: 0.696\n",
      "time t = 423\n",
      "training loss at last epoch: 0.696\n",
      "time t = 424\n",
      "training loss at last epoch: 0.695\n",
      "time t = 425\n",
      "training loss at last epoch: 0.694\n",
      "time t = 426\n",
      "training loss at last epoch: 0.696\n",
      "time t = 427\n",
      "training loss at last epoch: 0.696\n",
      "time t = 428\n",
      "training loss at last epoch: 0.699\n",
      "time t = 429\n",
      "training loss at last epoch: 0.697\n",
      "time t = 430\n",
      "training loss at last epoch: 0.696\n",
      "time t = 431\n",
      "training loss at last epoch: 0.698\n",
      "time t = 432\n",
      "training loss at last epoch: 0.696\n",
      "time t = 433\n",
      "training loss at last epoch: 0.696\n",
      "time t = 434\n",
      "training loss at last epoch: 0.696\n",
      "time t = 435\n",
      "training loss at last epoch: 0.694\n",
      "time t = 436\n",
      "training loss at last epoch: 0.699\n",
      "time t = 437\n",
      "training loss at last epoch: 0.694\n",
      "time t = 438\n",
      "training loss at last epoch: 0.698\n",
      "time t = 439\n",
      "training loss at last epoch: 0.696\n",
      "time t = 440\n",
      "training loss at last epoch: 0.695\n",
      "time t = 441\n",
      "training loss at last epoch: 0.695\n",
      "time t = 442\n",
      "training loss at last epoch: 0.694\n",
      "time t = 443\n",
      "training loss at last epoch: 0.695\n",
      "time t = 444\n",
      "training loss at last epoch: 0.694\n",
      "time t = 445\n",
      "training loss at last epoch: 0.698\n",
      "time t = 446\n",
      "training loss at last epoch: 0.696\n",
      "time t = 447\n",
      "training loss at last epoch: 0.694\n",
      "time t = 448\n",
      "training loss at last epoch: 0.693\n",
      "time t = 449\n",
      "training loss at last epoch: 0.695\n",
      "time t = 450\n",
      "training loss at last epoch: 0.699\n",
      "time t = 451\n",
      "training loss at last epoch: 0.695\n",
      "time t = 452\n",
      "training loss at last epoch: 0.694\n",
      "time t = 453\n",
      "training loss at last epoch: 0.694\n",
      "time t = 454\n",
      "training loss at last epoch: 0.694\n",
      "time t = 455\n",
      "training loss at last epoch: 0.695\n",
      "time t = 456\n",
      "training loss at last epoch: 0.695\n",
      "time t = 457\n",
      "training loss at last epoch: 0.695\n",
      "time t = 458\n",
      "training loss at last epoch: 0.695\n",
      "time t = 459\n",
      "training loss at last epoch: 0.694\n",
      "time t = 460\n",
      "training loss at last epoch: 0.694\n",
      "time t = 461\n",
      "training loss at last epoch: 0.694\n",
      "time t = 462\n",
      "training loss at last epoch: 0.696\n",
      "time t = 463\n",
      "training loss at last epoch: 0.692\n",
      "time t = 464\n",
      "training loss at last epoch: 0.694\n",
      "time t = 465\n",
      "training loss at last epoch: 0.694\n",
      "time t = 466\n",
      "training loss at last epoch: 0.695\n",
      "time t = 467\n",
      "training loss at last epoch: 0.695\n",
      "time t = 468\n",
      "training loss at last epoch: 0.695\n",
      "time t = 469\n",
      "training loss at last epoch: 0.694\n",
      "time t = 470\n",
      "training loss at last epoch: 0.695\n",
      "time t = 471\n",
      "training loss at last epoch: 0.695\n",
      "time t = 472\n",
      "training loss at last epoch: 0.695\n",
      "time t = 473\n",
      "training loss at last epoch: 0.696\n",
      "time t = 474\n",
      "training loss at last epoch: 0.697\n",
      "time t = 475\n",
      "training loss at last epoch: 0.695\n",
      "time t = 476\n",
      "training loss at last epoch: 0.694\n",
      "time t = 477\n",
      "training loss at last epoch: 0.698\n",
      "time t = 478\n",
      "training loss at last epoch: 0.696\n",
      "time t = 479\n",
      "training loss at last epoch: 0.693\n",
      "time t = 480\n",
      "training loss at last epoch: 0.696\n",
      "time t = 481\n",
      "training loss at last epoch: 0.695\n",
      "time t = 482\n",
      "training loss at last epoch: 0.697\n",
      "time t = 483\n",
      "training loss at last epoch: 0.700\n",
      "time t = 484\n",
      "training loss at last epoch: 0.695\n",
      "time t = 485\n",
      "training loss at last epoch: 0.694\n",
      "time t = 486\n",
      "training loss at last epoch: 0.698\n",
      "time t = 487\n",
      "training loss at last epoch: 0.695\n",
      "time t = 488\n",
      "training loss at last epoch: 0.695\n",
      "time t = 489\n",
      "training loss at last epoch: 0.697\n",
      "time t = 490\n",
      "training loss at last epoch: 0.694\n",
      "time t = 491\n",
      "training loss at last epoch: 0.694\n",
      "time t = 492\n",
      "training loss at last epoch: 0.695\n",
      "time t = 493\n",
      "training loss at last epoch: 0.695\n",
      "time t = 494\n",
      "training loss at last epoch: 0.696\n",
      "time t = 495\n",
      "training loss at last epoch: 0.693\n",
      "time t = 496\n",
      "training loss at last epoch: 0.695\n",
      "time t = 497\n",
      "training loss at last epoch: 0.694\n",
      "time t = 498\n",
      "training loss at last epoch: 0.697\n",
      "time t = 499\n",
      "training loss at last epoch: 0.695\n",
      "time t = 500\n",
      "training loss at last epoch: 0.695\n",
      "time t = 501\n",
      "training loss at last epoch: 0.695\n",
      "time t = 502\n",
      "training loss at last epoch: 0.694\n",
      "time t = 503\n",
      "training loss at last epoch: 0.694\n",
      "time t = 504\n",
      "training loss at last epoch: 0.695\n",
      "time t = 505\n",
      "training loss at last epoch: 0.695\n",
      "time t = 506\n",
      "training loss at last epoch: 0.696\n",
      "time t = 507\n",
      "training loss at last epoch: 0.697\n",
      "time t = 508\n",
      "training loss at last epoch: 0.695\n",
      "time t = 509\n",
      "training loss at last epoch: 0.695\n",
      "time t = 510\n",
      "training loss at last epoch: 0.696\n",
      "time t = 511\n",
      "training loss at last epoch: 0.697\n",
      "time t = 512\n",
      "training loss at last epoch: 0.696\n",
      "time t = 513\n",
      "training loss at last epoch: 0.700\n",
      "time t = 514\n",
      "training loss at last epoch: 0.694\n",
      "time t = 515\n",
      "training loss at last epoch: 0.695\n",
      "time t = 516\n",
      "training loss at last epoch: 0.696\n",
      "time t = 517\n",
      "training loss at last epoch: 0.696\n",
      "time t = 518\n",
      "training loss at last epoch: 0.694\n",
      "time t = 519\n",
      "training loss at last epoch: 0.695\n",
      "time t = 520\n",
      "training loss at last epoch: 0.695\n",
      "time t = 521\n",
      "training loss at last epoch: 0.697\n",
      "time t = 522\n",
      "training loss at last epoch: 0.695\n",
      "time t = 523\n",
      "training loss at last epoch: 0.695\n",
      "time t = 524\n",
      "training loss at last epoch: 0.696\n",
      "time t = 525\n",
      "training loss at last epoch: 0.696\n",
      "time t = 526\n",
      "training loss at last epoch: 0.696\n",
      "time t = 527\n",
      "training loss at last epoch: 0.697\n",
      "time t = 528\n",
      "training loss at last epoch: 0.702\n",
      "time t = 529\n",
      "training loss at last epoch: 0.696\n",
      "time t = 530\n",
      "training loss at last epoch: 0.694\n",
      "time t = 531\n",
      "training loss at last epoch: 0.696\n",
      "time t = 532\n",
      "training loss at last epoch: 0.694\n",
      "time t = 533\n",
      "training loss at last epoch: 0.696\n",
      "time t = 534\n",
      "training loss at last epoch: 0.695\n",
      "time t = 535\n",
      "training loss at last epoch: 0.697\n",
      "time t = 536\n",
      "training loss at last epoch: 0.694\n",
      "time t = 537\n",
      "training loss at last epoch: 0.694\n",
      "time t = 538\n",
      "training loss at last epoch: 0.694\n",
      "time t = 539\n",
      "training loss at last epoch: 0.697\n",
      "time t = 540\n",
      "training loss at last epoch: 0.699\n",
      "time t = 541\n",
      "training loss at last epoch: 0.694\n",
      "time t = 542\n",
      "training loss at last epoch: 0.696\n",
      "time t = 543\n",
      "training loss at last epoch: 0.695\n",
      "time t = 544\n",
      "training loss at last epoch: 0.696\n",
      "time t = 545\n",
      "training loss at last epoch: 0.697\n",
      "time t = 546\n",
      "training loss at last epoch: 0.695\n",
      "time t = 547\n",
      "training loss at last epoch: 0.696\n",
      "time t = 548\n",
      "training loss at last epoch: 0.697\n",
      "time t = 549\n",
      "training loss at last epoch: 0.695\n",
      "time t = 550\n",
      "training loss at last epoch: 0.694\n",
      "time t = 551\n",
      "training loss at last epoch: 0.696\n",
      "time t = 552\n",
      "training loss at last epoch: 0.693\n",
      "time t = 553\n",
      "training loss at last epoch: 0.697\n",
      "time t = 554\n",
      "training loss at last epoch: 0.696\n",
      "time t = 555\n",
      "training loss at last epoch: 0.695\n",
      "time t = 556\n",
      "training loss at last epoch: 0.694\n",
      "time t = 557\n",
      "training loss at last epoch: 0.697\n",
      "time t = 558\n",
      "training loss at last epoch: 0.696\n",
      "time t = 559\n",
      "training loss at last epoch: 0.694\n",
      "time t = 560\n",
      "training loss at last epoch: 0.695\n",
      "time t = 561\n",
      "training loss at last epoch: 0.695\n",
      "time t = 562\n",
      "training loss at last epoch: 0.697\n",
      "time t = 563\n",
      "training loss at last epoch: 0.696\n",
      "time t = 564\n",
      "training loss at last epoch: 0.696\n",
      "time t = 565\n",
      "training loss at last epoch: 0.695\n",
      "time t = 566\n",
      "training loss at last epoch: 0.696\n",
      "time t = 567\n",
      "training loss at last epoch: 0.696\n",
      "time t = 568\n",
      "training loss at last epoch: 0.695\n",
      "time t = 569\n",
      "training loss at last epoch: 0.694\n",
      "time t = 570\n",
      "training loss at last epoch: 0.694\n",
      "time t = 571\n",
      "training loss at last epoch: 0.696\n",
      "time t = 572\n",
      "training loss at last epoch: 0.695\n",
      "time t = 573\n",
      "training loss at last epoch: 0.696\n",
      "time t = 574\n",
      "training loss at last epoch: 0.697\n",
      "time t = 575\n",
      "training loss at last epoch: 0.694\n",
      "time t = 576\n",
      "training loss at last epoch: 0.696\n",
      "time t = 577\n",
      "training loss at last epoch: 0.697\n",
      "time t = 578\n",
      "training loss at last epoch: 0.695\n",
      "time t = 579\n",
      "training loss at last epoch: 0.696\n",
      "time t = 580\n",
      "training loss at last epoch: 0.695\n",
      "time t = 581\n",
      "training loss at last epoch: 0.696\n",
      "time t = 582\n",
      "training loss at last epoch: 0.697\n",
      "time t = 583\n",
      "training loss at last epoch: 0.697\n",
      "time t = 584\n",
      "training loss at last epoch: 0.697\n",
      "time t = 585\n",
      "training loss at last epoch: 0.695\n",
      "time t = 586\n",
      "training loss at last epoch: 0.695\n",
      "time t = 587\n",
      "training loss at last epoch: 0.695\n",
      "time t = 588\n",
      "training loss at last epoch: 0.696\n",
      "time t = 589\n",
      "training loss at last epoch: 0.695\n",
      "time t = 590\n",
      "training loss at last epoch: 0.696\n",
      "time t = 591\n",
      "training loss at last epoch: 0.695\n",
      "time t = 592\n",
      "training loss at last epoch: 0.695\n",
      "time t = 593\n",
      "training loss at last epoch: 0.695\n",
      "time t = 594\n",
      "training loss at last epoch: 0.697\n",
      "time t = 595\n",
      "training loss at last epoch: 0.696\n",
      "time t = 596\n",
      "training loss at last epoch: 0.696\n",
      "time t = 597\n",
      "training loss at last epoch: 0.696\n",
      "time t = 598\n",
      "training loss at last epoch: 0.698\n",
      "time t = 599\n",
      "training loss at last epoch: 0.695\n",
      "time t = 600\n",
      "training loss at last epoch: 0.697\n",
      "time t = 601\n",
      "training loss at last epoch: 0.695\n",
      "time t = 602\n",
      "training loss at last epoch: 0.694\n",
      "time t = 603\n",
      "training loss at last epoch: 0.694\n",
      "time t = 604\n",
      "training loss at last epoch: 0.696\n",
      "time t = 605\n",
      "training loss at last epoch: 0.695\n",
      "time t = 606\n",
      "training loss at last epoch: 0.699\n",
      "time t = 607\n",
      "training loss at last epoch: 0.695\n",
      "time t = 608\n",
      "training loss at last epoch: 0.695\n",
      "time t = 609\n",
      "training loss at last epoch: 0.695\n",
      "time t = 610\n",
      "training loss at last epoch: 0.696\n",
      "time t = 611\n",
      "training loss at last epoch: 0.694\n",
      "time t = 612\n",
      "training loss at last epoch: 0.696\n",
      "time t = 613\n",
      "training loss at last epoch: 0.696\n",
      "time t = 614\n",
      "training loss at last epoch: 0.697\n",
      "time t = 615\n",
      "training loss at last epoch: 0.695\n",
      "time t = 616\n",
      "training loss at last epoch: 0.697\n",
      "time t = 617\n",
      "training loss at last epoch: 0.697\n",
      "time t = 618\n",
      "training loss at last epoch: 0.695\n",
      "time t = 619\n",
      "training loss at last epoch: 0.696\n",
      "time t = 620\n",
      "training loss at last epoch: 0.696\n",
      "time t = 621\n",
      "training loss at last epoch: 0.694\n",
      "time t = 622\n",
      "training loss at last epoch: 0.695\n",
      "time t = 623\n",
      "training loss at last epoch: 0.696\n",
      "time t = 624\n",
      "training loss at last epoch: 0.695\n",
      "time t = 625\n",
      "training loss at last epoch: 0.695\n",
      "time t = 626\n",
      "training loss at last epoch: 0.696\n",
      "time t = 627\n",
      "training loss at last epoch: 0.697\n",
      "time t = 628\n",
      "training loss at last epoch: 0.695\n",
      "time t = 629\n",
      "training loss at last epoch: 0.695\n",
      "time t = 630\n",
      "training loss at last epoch: 0.695\n",
      "time t = 631\n",
      "training loss at last epoch: 0.696\n",
      "time t = 632\n",
      "training loss at last epoch: 0.695\n",
      "time t = 633\n",
      "training loss at last epoch: 0.696\n",
      "time t = 634\n",
      "training loss at last epoch: 0.696\n",
      "time t = 635\n",
      "training loss at last epoch: 0.695\n",
      "time t = 636\n",
      "training loss at last epoch: 0.695\n",
      "time t = 637\n",
      "training loss at last epoch: 0.694\n",
      "time t = 638\n",
      "training loss at last epoch: 0.695\n",
      "time t = 639\n",
      "training loss at last epoch: 0.695\n",
      "time t = 640\n",
      "training loss at last epoch: 0.695\n",
      "time t = 641\n",
      "training loss at last epoch: 0.696\n",
      "time t = 642\n",
      "training loss at last epoch: 0.697\n",
      "time t = 643\n",
      "training loss at last epoch: 0.696\n",
      "time t = 644\n",
      "training loss at last epoch: 0.695\n",
      "time t = 645\n",
      "training loss at last epoch: 0.695\n",
      "time t = 646\n",
      "training loss at last epoch: 0.695\n",
      "time t = 647\n",
      "training loss at last epoch: 0.697\n",
      "time t = 648\n",
      "training loss at last epoch: 0.695\n",
      "time t = 649\n",
      "training loss at last epoch: 0.695\n",
      "time t = 650\n",
      "training loss at last epoch: 0.695\n",
      "time t = 651\n",
      "training loss at last epoch: 0.695\n",
      "time t = 652\n",
      "training loss at last epoch: 0.696\n",
      "time t = 653\n",
      "training loss at last epoch: 0.697\n",
      "time t = 654\n",
      "training loss at last epoch: 0.695\n",
      "time t = 655\n",
      "training loss at last epoch: 0.696\n",
      "time t = 656\n",
      "training loss at last epoch: 0.695\n",
      "time t = 657\n",
      "training loss at last epoch: 0.694\n",
      "time t = 658\n",
      "training loss at last epoch: 0.695\n",
      "time t = 659\n",
      "training loss at last epoch: 0.695\n",
      "time t = 660\n",
      "training loss at last epoch: 0.696\n",
      "time t = 661\n",
      "training loss at last epoch: 0.697\n",
      "time t = 662\n",
      "training loss at last epoch: 0.696\n",
      "time t = 663\n",
      "training loss at last epoch: 0.696\n",
      "time t = 664\n",
      "training loss at last epoch: 0.696\n",
      "time t = 665\n",
      "training loss at last epoch: 0.696\n",
      "time t = 666\n",
      "training loss at last epoch: 0.696\n",
      "time t = 667\n",
      "training loss at last epoch: 0.697\n",
      "time t = 668\n",
      "training loss at last epoch: 0.695\n",
      "time t = 669\n",
      "training loss at last epoch: 0.696\n",
      "time t = 670\n",
      "training loss at last epoch: 0.697\n",
      "time t = 671\n",
      "training loss at last epoch: 0.695\n",
      "time t = 672\n",
      "training loss at last epoch: 0.695\n",
      "time t = 673\n",
      "training loss at last epoch: 0.696\n",
      "time t = 674\n",
      "training loss at last epoch: 0.696\n",
      "time t = 675\n",
      "training loss at last epoch: 0.696\n",
      "time t = 676\n",
      "training loss at last epoch: 0.697\n",
      "time t = 677\n",
      "training loss at last epoch: 0.697\n",
      "time t = 678\n",
      "training loss at last epoch: 0.697\n",
      "time t = 679\n",
      "training loss at last epoch: 0.696\n",
      "time t = 680\n",
      "training loss at last epoch: 0.696\n",
      "time t = 681\n",
      "training loss at last epoch: 0.695\n",
      "time t = 682\n",
      "training loss at last epoch: 0.697\n",
      "time t = 683\n",
      "training loss at last epoch: 0.695\n",
      "time t = 684\n",
      "training loss at last epoch: 0.696\n",
      "time t = 685\n",
      "training loss at last epoch: 0.696\n",
      "time t = 686\n",
      "training loss at last epoch: 0.697\n",
      "time t = 687\n",
      "training loss at last epoch: 0.696\n",
      "time t = 688\n",
      "training loss at last epoch: 0.698\n",
      "time t = 689\n",
      "training loss at last epoch: 0.697\n",
      "time t = 690\n",
      "training loss at last epoch: 0.696\n",
      "time t = 691\n",
      "training loss at last epoch: 0.696\n",
      "time t = 692\n",
      "training loss at last epoch: 0.696\n",
      "time t = 693\n",
      "training loss at last epoch: 0.695\n",
      "time t = 694\n",
      "training loss at last epoch: 0.694\n",
      "time t = 695\n",
      "training loss at last epoch: 0.695\n",
      "time t = 696\n",
      "training loss at last epoch: 0.696\n",
      "time t = 697\n",
      "training loss at last epoch: 0.696\n",
      "time t = 698\n",
      "training loss at last epoch: 0.696\n",
      "time t = 699\n",
      "training loss at last epoch: 0.697\n",
      "time t = 700\n",
      "training loss at last epoch: 0.696\n",
      "time t = 701\n",
      "training loss at last epoch: 0.695\n",
      "time t = 702\n",
      "training loss at last epoch: 0.696\n",
      "time t = 703\n",
      "training loss at last epoch: 0.696\n",
      "time t = 704\n",
      "training loss at last epoch: 0.696\n",
      "time t = 705\n",
      "training loss at last epoch: 0.696\n",
      "time t = 706\n",
      "training loss at last epoch: 0.698\n",
      "time t = 707\n",
      "training loss at last epoch: 0.694\n",
      "time t = 708\n",
      "training loss at last epoch: 0.699\n",
      "time t = 709\n",
      "training loss at last epoch: 0.695\n",
      "time t = 710\n",
      "training loss at last epoch: 0.696\n",
      "time t = 711\n",
      "training loss at last epoch: 0.696\n",
      "time t = 712\n",
      "training loss at last epoch: 0.695\n",
      "time t = 713\n",
      "training loss at last epoch: 0.698\n",
      "time t = 714\n",
      "training loss at last epoch: 0.695\n",
      "time t = 715\n",
      "training loss at last epoch: 0.697\n",
      "time t = 716\n",
      "training loss at last epoch: 0.695\n",
      "time t = 717\n",
      "training loss at last epoch: 0.695\n",
      "time t = 718\n",
      "training loss at last epoch: 0.694\n",
      "time t = 719\n",
      "training loss at last epoch: 0.697\n",
      "time t = 720\n",
      "training loss at last epoch: 0.694\n",
      "time t = 721\n",
      "training loss at last epoch: 0.696\n",
      "time t = 722\n",
      "training loss at last epoch: 0.695\n",
      "time t = 723\n",
      "training loss at last epoch: 0.694\n",
      "time t = 724\n",
      "training loss at last epoch: 0.696\n",
      "time t = 725\n",
      "training loss at last epoch: 0.697\n",
      "time t = 726\n",
      "training loss at last epoch: 0.696\n",
      "time t = 727\n",
      "training loss at last epoch: 0.696\n",
      "time t = 728\n",
      "training loss at last epoch: 0.698\n",
      "time t = 729\n",
      "training loss at last epoch: 0.695\n",
      "time t = 730\n",
      "training loss at last epoch: 0.697\n",
      "time t = 731\n",
      "training loss at last epoch: 0.699\n",
      "time t = 732\n",
      "training loss at last epoch: 0.695\n",
      "time t = 733\n",
      "training loss at last epoch: 0.697\n",
      "time t = 734\n",
      "training loss at last epoch: 0.694\n",
      "time t = 735\n",
      "training loss at last epoch: 0.696\n",
      "time t = 736\n",
      "training loss at last epoch: 0.694\n",
      "time t = 737\n",
      "training loss at last epoch: 0.695\n",
      "time t = 738\n",
      "training loss at last epoch: 0.694\n",
      "time t = 739\n",
      "training loss at last epoch: 0.696\n",
      "time t = 740\n",
      "training loss at last epoch: 0.696\n",
      "time t = 741\n",
      "training loss at last epoch: 0.696\n",
      "time t = 742\n",
      "training loss at last epoch: 0.696\n",
      "time t = 743\n",
      "training loss at last epoch: 0.697\n",
      "time t = 744\n",
      "training loss at last epoch: 0.697\n",
      "time t = 745\n",
      "training loss at last epoch: 0.696\n",
      "time t = 746\n",
      "training loss at last epoch: 0.695\n",
      "time t = 747\n",
      "training loss at last epoch: 0.696\n",
      "time t = 748\n",
      "training loss at last epoch: 0.696\n",
      "time t = 749\n",
      "training loss at last epoch: 0.696\n",
      "time t = 750\n",
      "training loss at last epoch: 0.698\n",
      "time t = 751\n",
      "training loss at last epoch: 0.695\n",
      "time t = 752\n",
      "training loss at last epoch: 0.694\n",
      "time t = 753\n",
      "training loss at last epoch: 0.697\n",
      "time t = 754\n",
      "training loss at last epoch: 0.696\n",
      "time t = 755\n",
      "training loss at last epoch: 0.696\n",
      "time t = 756\n",
      "training loss at last epoch: 0.697\n",
      "time t = 757\n",
      "training loss at last epoch: 0.698\n",
      "time t = 758\n",
      "training loss at last epoch: 0.695\n",
      "time t = 759\n",
      "training loss at last epoch: 0.698\n",
      "time t = 760\n",
      "training loss at last epoch: 0.696\n",
      "time t = 761\n",
      "training loss at last epoch: 0.695\n",
      "time t = 762\n",
      "training loss at last epoch: 0.696\n",
      "time t = 763\n",
      "training loss at last epoch: 0.695\n",
      "time t = 764\n",
      "training loss at last epoch: 0.697\n",
      "time t = 765\n",
      "training loss at last epoch: 0.697\n",
      "time t = 766\n",
      "training loss at last epoch: 0.695\n",
      "time t = 767\n",
      "training loss at last epoch: 0.696\n",
      "time t = 768\n",
      "training loss at last epoch: 0.696\n",
      "time t = 769\n",
      "training loss at last epoch: 0.697\n",
      "time t = 770\n",
      "training loss at last epoch: 0.695\n",
      "time t = 771\n",
      "training loss at last epoch: 0.697\n",
      "time t = 772\n",
      "training loss at last epoch: 0.696\n",
      "time t = 773\n",
      "training loss at last epoch: 0.695\n",
      "time t = 774\n",
      "training loss at last epoch: 0.696\n",
      "time t = 775\n",
      "training loss at last epoch: 0.695\n",
      "time t = 776\n",
      "training loss at last epoch: 0.696\n",
      "time t = 777\n",
      "training loss at last epoch: 0.696\n",
      "time t = 778\n",
      "training loss at last epoch: 0.696\n",
      "time t = 779\n",
      "training loss at last epoch: 0.697\n",
      "time t = 780\n",
      "training loss at last epoch: 0.696\n",
      "time t = 781\n",
      "training loss at last epoch: 0.695\n",
      "time t = 782\n",
      "training loss at last epoch: 0.696\n",
      "time t = 783\n",
      "training loss at last epoch: 0.696\n",
      "time t = 784\n",
      "training loss at last epoch: 0.696\n",
      "time t = 785\n",
      "training loss at last epoch: 0.696\n",
      "time t = 786\n",
      "training loss at last epoch: 0.695\n",
      "time t = 787\n",
      "training loss at last epoch: 0.696\n",
      "time t = 788\n",
      "training loss at last epoch: 0.695\n",
      "time t = 789\n",
      "training loss at last epoch: 0.694\n",
      "time t = 790\n",
      "training loss at last epoch: 0.696\n",
      "time t = 791\n",
      "training loss at last epoch: 0.695\n",
      "time t = 792\n",
      "training loss at last epoch: 0.695\n",
      "time t = 793\n",
      "training loss at last epoch: 0.697\n",
      "time t = 794\n",
      "training loss at last epoch: 0.696\n",
      "time t = 795\n",
      "training loss at last epoch: 0.695\n",
      "time t = 796\n",
      "training loss at last epoch: 0.695\n",
      "time t = 797\n",
      "training loss at last epoch: 0.696\n",
      "time t = 798\n",
      "training loss at last epoch: 0.696\n",
      "time t = 799\n",
      "training loss at last epoch: 0.695\n",
      "time t = 800\n",
      "training loss at last epoch: 0.697\n",
      "time t = 801\n",
      "training loss at last epoch: 0.694\n",
      "time t = 802\n",
      "training loss at last epoch: 0.696\n",
      "time t = 803\n",
      "training loss at last epoch: 0.695\n",
      "time t = 804\n",
      "training loss at last epoch: 0.697\n",
      "time t = 805\n",
      "training loss at last epoch: 0.694\n",
      "time t = 806\n",
      "training loss at last epoch: 0.698\n",
      "time t = 807\n",
      "training loss at last epoch: 0.695\n",
      "time t = 808\n",
      "training loss at last epoch: 0.695\n",
      "time t = 809\n",
      "training loss at last epoch: 0.697\n",
      "time t = 810\n",
      "training loss at last epoch: 0.695\n",
      "time t = 811\n",
      "training loss at last epoch: 0.696\n",
      "time t = 812\n",
      "training loss at last epoch: 0.695\n",
      "time t = 813\n",
      "training loss at last epoch: 0.696\n",
      "time t = 814\n",
      "training loss at last epoch: 0.696\n",
      "time t = 815\n",
      "training loss at last epoch: 0.696\n",
      "time t = 816\n",
      "training loss at last epoch: 0.695\n",
      "time t = 817\n",
      "training loss at last epoch: 0.696\n",
      "time t = 818\n",
      "training loss at last epoch: 0.695\n",
      "time t = 819\n",
      "training loss at last epoch: 0.697\n",
      "time t = 820\n",
      "training loss at last epoch: 0.695\n",
      "time t = 821\n",
      "training loss at last epoch: 0.696\n",
      "time t = 822\n",
      "training loss at last epoch: 0.697\n",
      "time t = 823\n",
      "training loss at last epoch: 0.694\n",
      "time t = 824\n",
      "training loss at last epoch: 0.698\n",
      "time t = 825\n",
      "training loss at last epoch: 0.697\n",
      "time t = 826\n",
      "training loss at last epoch: 0.697\n",
      "time t = 827\n",
      "training loss at last epoch: 0.696\n",
      "time t = 828\n",
      "training loss at last epoch: 0.699\n",
      "time t = 829\n",
      "training loss at last epoch: 0.695\n",
      "time t = 830\n",
      "training loss at last epoch: 0.697\n",
      "time t = 831\n",
      "training loss at last epoch: 0.695\n",
      "time t = 832\n",
      "training loss at last epoch: 0.696\n",
      "time t = 833\n",
      "training loss at last epoch: 0.697\n",
      "time t = 834\n",
      "training loss at last epoch: 0.696\n",
      "time t = 835\n",
      "training loss at last epoch: 0.697\n",
      "time t = 836\n",
      "training loss at last epoch: 0.696\n",
      "time t = 837\n",
      "training loss at last epoch: 0.695\n",
      "time t = 838\n",
      "training loss at last epoch: 0.695\n",
      "time t = 839\n",
      "training loss at last epoch: 0.695\n",
      "time t = 840\n",
      "training loss at last epoch: 0.696\n",
      "time t = 841\n",
      "training loss at last epoch: 0.694\n",
      "time t = 842\n",
      "training loss at last epoch: 0.695\n",
      "time t = 843\n",
      "training loss at last epoch: 0.694\n",
      "time t = 844\n",
      "training loss at last epoch: 0.696\n",
      "time t = 845\n",
      "training loss at last epoch: 0.695\n",
      "time t = 846\n",
      "training loss at last epoch: 0.695\n",
      "time t = 847\n",
      "training loss at last epoch: 0.695\n",
      "time t = 848\n",
      "training loss at last epoch: 0.698\n",
      "time t = 849\n",
      "training loss at last epoch: 0.696\n",
      "time t = 850\n",
      "training loss at last epoch: 0.695\n",
      "time t = 851\n",
      "training loss at last epoch: 0.696\n",
      "time t = 852\n",
      "training loss at last epoch: 0.695\n",
      "time t = 853\n",
      "training loss at last epoch: 0.696\n",
      "time t = 854\n",
      "training loss at last epoch: 0.698\n",
      "time t = 855\n",
      "training loss at last epoch: 0.695\n",
      "time t = 856\n",
      "training loss at last epoch: 0.696\n",
      "time t = 857\n",
      "training loss at last epoch: 0.695\n",
      "time t = 858\n",
      "training loss at last epoch: 0.699\n",
      "time t = 859\n",
      "training loss at last epoch: 0.695\n",
      "time t = 860\n",
      "training loss at last epoch: 0.694\n",
      "time t = 861\n",
      "training loss at last epoch: 0.694\n",
      "time t = 862\n",
      "training loss at last epoch: 0.697\n",
      "time t = 863\n",
      "training loss at last epoch: 0.695\n",
      "time t = 864\n",
      "training loss at last epoch: 0.697\n",
      "time t = 865\n",
      "training loss at last epoch: 0.696\n",
      "time t = 866\n",
      "training loss at last epoch: 0.696\n",
      "time t = 867\n",
      "training loss at last epoch: 0.696\n",
      "time t = 868\n",
      "training loss at last epoch: 0.694\n",
      "time t = 869\n",
      "training loss at last epoch: 0.697\n",
      "time t = 870\n",
      "training loss at last epoch: 0.696\n",
      "time t = 871\n",
      "training loss at last epoch: 0.695\n",
      "time t = 872\n",
      "training loss at last epoch: 0.695\n",
      "time t = 873\n",
      "training loss at last epoch: 0.695\n",
      "time t = 874\n",
      "training loss at last epoch: 0.697\n",
      "time t = 875\n",
      "training loss at last epoch: 0.695\n",
      "time t = 876\n",
      "training loss at last epoch: 0.696\n",
      "time t = 877\n",
      "training loss at last epoch: 0.696\n",
      "time t = 878\n",
      "training loss at last epoch: 0.696\n",
      "time t = 879\n",
      "training loss at last epoch: 0.695\n",
      "time t = 880\n",
      "training loss at last epoch: 0.695\n",
      "time t = 881\n",
      "training loss at last epoch: 0.696\n",
      "time t = 882\n",
      "training loss at last epoch: 0.697\n",
      "time t = 883\n",
      "training loss at last epoch: 0.697\n",
      "time t = 884\n",
      "training loss at last epoch: 0.695\n",
      "time t = 885\n",
      "training loss at last epoch: 0.697\n",
      "time t = 886\n",
      "training loss at last epoch: 0.696\n",
      "time t = 887\n",
      "training loss at last epoch: 0.696\n",
      "time t = 888\n",
      "training loss at last epoch: 0.696\n",
      "time t = 889\n",
      "training loss at last epoch: 0.696\n",
      "time t = 890\n",
      "training loss at last epoch: 0.696\n",
      "time t = 891\n",
      "training loss at last epoch: 0.696\n",
      "time t = 892\n",
      "training loss at last epoch: 0.695\n",
      "time t = 893\n",
      "training loss at last epoch: 0.695\n",
      "time t = 894\n",
      "training loss at last epoch: 0.696\n",
      "time t = 895\n",
      "training loss at last epoch: 0.696\n",
      "time t = 896\n",
      "training loss at last epoch: 0.695\n",
      "time t = 897\n",
      "training loss at last epoch: 0.695\n",
      "time t = 898\n",
      "training loss at last epoch: 0.699\n",
      "time t = 899\n",
      "training loss at last epoch: 0.696\n",
      "time t = 900\n",
      "training loss at last epoch: 0.696\n",
      "time t = 901\n",
      "training loss at last epoch: 0.696\n",
      "time t = 902\n",
      "training loss at last epoch: 0.696\n",
      "time t = 903\n",
      "training loss at last epoch: 0.696\n",
      "time t = 904\n",
      "training loss at last epoch: 0.696\n",
      "time t = 905\n",
      "training loss at last epoch: 0.696\n",
      "time t = 906\n",
      "training loss at last epoch: 0.696\n",
      "time t = 907\n",
      "training loss at last epoch: 0.696\n",
      "time t = 908\n",
      "training loss at last epoch: 0.697\n",
      "time t = 909\n",
      "training loss at last epoch: 0.695\n",
      "time t = 910\n",
      "training loss at last epoch: 0.696\n",
      "time t = 911\n",
      "training loss at last epoch: 0.696\n",
      "time t = 912\n",
      "training loss at last epoch: 0.696\n",
      "time t = 913\n",
      "training loss at last epoch: 0.696\n",
      "time t = 914\n",
      "training loss at last epoch: 0.695\n",
      "time t = 915\n",
      "training loss at last epoch: 0.696\n",
      "time t = 916\n",
      "training loss at last epoch: 0.696\n",
      "time t = 917\n",
      "training loss at last epoch: 0.698\n",
      "time t = 918\n",
      "training loss at last epoch: 0.696\n",
      "time t = 919\n",
      "training loss at last epoch: 0.696\n",
      "time t = 920\n",
      "training loss at last epoch: 0.696\n",
      "time t = 921\n",
      "training loss at last epoch: 0.695\n",
      "time t = 922\n",
      "training loss at last epoch: 0.696\n",
      "time t = 923\n",
      "training loss at last epoch: 0.695\n",
      "time t = 924\n",
      "training loss at last epoch: 0.697\n",
      "time t = 925\n",
      "training loss at last epoch: 0.697\n",
      "time t = 926\n",
      "training loss at last epoch: 0.695\n",
      "time t = 927\n",
      "training loss at last epoch: 0.695\n",
      "time t = 928\n",
      "training loss at last epoch: 0.695\n",
      "time t = 929\n",
      "training loss at last epoch: 0.696\n",
      "time t = 930\n",
      "training loss at last epoch: 0.696\n",
      "time t = 931\n",
      "training loss at last epoch: 0.697\n",
      "time t = 932\n",
      "training loss at last epoch: 0.694\n",
      "time t = 933\n",
      "training loss at last epoch: 0.696\n",
      "time t = 934\n",
      "training loss at last epoch: 0.695\n",
      "time t = 935\n",
      "training loss at last epoch: 0.695\n",
      "time t = 936\n",
      "training loss at last epoch: 0.696\n",
      "time t = 937\n",
      "training loss at last epoch: 0.696\n",
      "time t = 938\n",
      "training loss at last epoch: 0.695\n",
      "time t = 939\n",
      "training loss at last epoch: 0.695\n",
      "time t = 940\n",
      "training loss at last epoch: 0.695\n",
      "time t = 941\n",
      "training loss at last epoch: 0.695\n",
      "time t = 942\n",
      "training loss at last epoch: 0.695\n",
      "time t = 943\n",
      "training loss at last epoch: 0.696\n",
      "time t = 944\n",
      "training loss at last epoch: 0.695\n",
      "time t = 945\n",
      "training loss at last epoch: 0.695\n",
      "time t = 946\n",
      "training loss at last epoch: 0.695\n",
      "time t = 947\n",
      "training loss at last epoch: 0.697\n",
      "time t = 948\n",
      "training loss at last epoch: 0.695\n",
      "time t = 949\n",
      "training loss at last epoch: 0.697\n",
      "time t = 950\n",
      "training loss at last epoch: 0.695\n",
      "time t = 951\n",
      "training loss at last epoch: 0.697\n",
      "time t = 952\n",
      "training loss at last epoch: 0.697\n",
      "time t = 953\n",
      "training loss at last epoch: 0.696\n",
      "time t = 954\n",
      "training loss at last epoch: 0.695\n",
      "time t = 955\n",
      "training loss at last epoch: 0.695\n",
      "time t = 956\n",
      "training loss at last epoch: 0.695\n",
      "time t = 957\n",
      "training loss at last epoch: 0.695\n",
      "time t = 958\n",
      "training loss at last epoch: 0.696\n",
      "time t = 959\n",
      "training loss at last epoch: 0.697\n",
      "time t = 960\n",
      "training loss at last epoch: 0.697\n",
      "time t = 961\n",
      "training loss at last epoch: 0.696\n",
      "time t = 962\n",
      "training loss at last epoch: 0.696\n",
      "time t = 963\n",
      "training loss at last epoch: 0.695\n",
      "time t = 964\n",
      "training loss at last epoch: 0.695\n",
      "time t = 965\n",
      "training loss at last epoch: 0.695\n",
      "time t = 966\n",
      "training loss at last epoch: 0.697\n",
      "time t = 967\n",
      "training loss at last epoch: 0.694\n",
      "time t = 968\n",
      "training loss at last epoch: 0.698\n",
      "time t = 969\n",
      "training loss at last epoch: 0.696\n",
      "time t = 970\n",
      "training loss at last epoch: 0.696\n",
      "time t = 971\n",
      "training loss at last epoch: 0.696\n",
      "time t = 972\n",
      "training loss at last epoch: 0.696\n",
      "time t = 973\n",
      "training loss at last epoch: 0.697\n",
      "time t = 974\n",
      "training loss at last epoch: 0.696\n",
      "time t = 975\n",
      "training loss at last epoch: 0.696\n",
      "time t = 976\n",
      "training loss at last epoch: 0.695\n",
      "time t = 977\n",
      "training loss at last epoch: 0.696\n",
      "time t = 978\n",
      "training loss at last epoch: 0.696\n",
      "time t = 979\n",
      "training loss at last epoch: 0.697\n",
      "time t = 980\n",
      "training loss at last epoch: 0.696\n",
      "time t = 981\n",
      "training loss at last epoch: 0.695\n",
      "time t = 982\n",
      "training loss at last epoch: 0.696\n",
      "time t = 983\n",
      "training loss at last epoch: 0.696\n",
      "time t = 984\n",
      "training loss at last epoch: 0.696\n",
      "time t = 985\n",
      "training loss at last epoch: 0.696\n",
      "time t = 986\n",
      "training loss at last epoch: 0.695\n",
      "time t = 987\n",
      "training loss at last epoch: 0.696\n",
      "time t = 988\n",
      "training loss at last epoch: 0.697\n",
      "time t = 989\n",
      "training loss at last epoch: 0.696\n",
      "time t = 990\n",
      "training loss at last epoch: 0.695\n",
      "time t = 991\n",
      "training loss at last epoch: 0.695\n",
      "time t = 992\n",
      "training loss at last epoch: 0.696\n",
      "time t = 993\n",
      "training loss at last epoch: 0.695\n",
      "time t = 994\n",
      "training loss at last epoch: 0.696\n",
      "time t = 995\n",
      "training loss at last epoch: 0.697\n",
      "time t = 996\n",
      "training loss at last epoch: 0.696\n",
      "time t = 997\n",
      "training loss at last epoch: 0.696\n",
      "time t = 998\n",
      "training loss at last epoch: 0.696\n",
      "time t = 999\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1000\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1001\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1002\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1003\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1004\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1005\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1006\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1007\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1008\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1009\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1010\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1011\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1012\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1013\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1014\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1015\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1016\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1017\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1018\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1019\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1020\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1021\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1022\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1023\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1024\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1025\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1026\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1027\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1028\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1029\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1030\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1031\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1032\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1033\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1034\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1035\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1036\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1037\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1038\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1039\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1040\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1041\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1042\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1043\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1044\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1045\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1046\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1047\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1048\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1049\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1050\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1051\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1052\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1053\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1054\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1055\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1056\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1057\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1058\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1059\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1060\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1061\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1062\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1063\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1064\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1065\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1066\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1067\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1068\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1069\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1070\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1071\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1072\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1073\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1074\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1075\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1076\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1077\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1078\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1079\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1080\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1081\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1082\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1083\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1084\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1085\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1086\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1087\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1088\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1089\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1090\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1091\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1092\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1093\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1094\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1095\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1096\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1097\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1098\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1099\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1100\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1101\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1102\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1103\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1104\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1105\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1106\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1107\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1108\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1109\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1110\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1111\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1112\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1113\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1114\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1115\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1116\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1117\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1118\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1119\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1120\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1121\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1122\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1123\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1124\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1125\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1126\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1127\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1128\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1129\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1130\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1131\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1132\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1133\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1134\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1135\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1136\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1137\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1138\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1139\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1140\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1141\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1142\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1143\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1144\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1145\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1146\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1147\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1148\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1149\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1150\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1151\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1152\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1153\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1154\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1155\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1156\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1157\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1158\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1159\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1160\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1161\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1162\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1163\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1164\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1165\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1166\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1167\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1168\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1169\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1170\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1171\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1172\n",
      "training loss at last epoch: 0.694\n",
      "time t = 1173\n",
      "training loss at last epoch: 0.698\n",
      "time t = 1174\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1175\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1176\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1177\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1178\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1179\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1180\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1181\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1182\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1183\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1184\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1185\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1186\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1187\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1188\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1189\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1190\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1191\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1192\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1193\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1194\n",
      "training loss at last epoch: 0.697\n",
      "time t = 1195\n",
      "training loss at last epoch: 0.696\n",
      "time t = 1196\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1197\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1198\n",
      "training loss at last epoch: 0.695\n",
      "time t = 1199\n",
      "training loss at last epoch: 0.696\n",
      "seed = 1\n",
      "time t = 20\n",
      "training loss at last epoch: 0.000\n",
      "time t = 21\n",
      "training loss at last epoch: 0.761\n",
      "time t = 22\n",
      "training loss at last epoch: 0.780\n",
      "time t = 23\n",
      "training loss at last epoch: 0.759\n",
      "time t = 24\n",
      "training loss at last epoch: 0.737\n",
      "time t = 25\n",
      "training loss at last epoch: 0.768\n",
      "time t = 26\n",
      "training loss at last epoch: 0.736\n",
      "time t = 27\n",
      "training loss at last epoch: 0.749\n",
      "time t = 28\n",
      "training loss at last epoch: 0.734\n",
      "time t = 29\n",
      "training loss at last epoch: 0.722\n",
      "time t = 30\n",
      "training loss at last epoch: 0.724\n",
      "time t = 31\n",
      "training loss at last epoch: 0.733\n",
      "time t = 32\n",
      "training loss at last epoch: 0.678\n",
      "time t = 33\n",
      "training loss at last epoch: 0.692\n",
      "time t = 34\n",
      "training loss at last epoch: 0.692\n",
      "time t = 35\n",
      "training loss at last epoch: 0.695\n",
      "time t = 36\n",
      "training loss at last epoch: 0.681\n",
      "time t = 37\n",
      "training loss at last epoch: 0.690\n",
      "time t = 38\n",
      "training loss at last epoch: 0.686\n",
      "time t = 39\n",
      "training loss at last epoch: 0.690\n",
      "time t = 40\n",
      "training loss at last epoch: 0.677\n",
      "time t = 41\n",
      "training loss at last epoch: 0.684\n",
      "time t = 42\n",
      "training loss at last epoch: 0.686\n",
      "time t = 43\n",
      "training loss at last epoch: 0.692\n",
      "time t = 44\n",
      "training loss at last epoch: 0.687\n",
      "time t = 45\n",
      "training loss at last epoch: 0.691\n",
      "time t = 46\n",
      "training loss at last epoch: 0.693\n",
      "time t = 47\n",
      "training loss at last epoch: 0.687\n",
      "time t = 48\n",
      "training loss at last epoch: 0.690\n",
      "time t = 49\n",
      "training loss at last epoch: 0.691\n",
      "time t = 50\n",
      "training loss at last epoch: 0.689\n",
      "time t = 51\n",
      "training loss at last epoch: 0.683\n",
      "time t = 52\n",
      "training loss at last epoch: 0.683\n",
      "time t = 53\n",
      "training loss at last epoch: 0.686\n",
      "time t = 54\n",
      "training loss at last epoch: 0.684\n",
      "time t = 55\n",
      "training loss at last epoch: 0.678\n",
      "time t = 56\n",
      "training loss at last epoch: 0.681\n",
      "time t = 57\n",
      "training loss at last epoch: 0.682\n",
      "time t = 58\n",
      "training loss at last epoch: 0.674\n",
      "time t = 59\n",
      "training loss at last epoch: 0.678\n",
      "time t = 60\n",
      "training loss at last epoch: 0.673\n",
      "time t = 61\n",
      "training loss at last epoch: 0.677\n",
      "time t = 62\n",
      "training loss at last epoch: 0.675\n",
      "time t = 63\n",
      "training loss at last epoch: 0.681\n",
      "time t = 64\n",
      "training loss at last epoch: 0.688\n",
      "time t = 65\n",
      "training loss at last epoch: 0.684\n",
      "time t = 66\n",
      "training loss at last epoch: 0.694\n",
      "time t = 67\n",
      "training loss at last epoch: 0.685\n",
      "time t = 68\n",
      "training loss at last epoch: 0.692\n",
      "time t = 69\n",
      "training loss at last epoch: 0.689\n",
      "time t = 70\n",
      "training loss at last epoch: 0.688\n",
      "time t = 71\n",
      "training loss at last epoch: 0.700\n",
      "time t = 72\n",
      "training loss at last epoch: 0.690\n",
      "time t = 73\n",
      "training loss at last epoch: 0.695\n",
      "time t = 74\n",
      "training loss at last epoch: 0.691\n",
      "time t = 75\n",
      "training loss at last epoch: 0.691\n",
      "time t = 76\n",
      "training loss at last epoch: 0.689\n",
      "time t = 77\n",
      "training loss at last epoch: 0.690\n",
      "time t = 78\n",
      "training loss at last epoch: 0.691\n",
      "time t = 79\n",
      "training loss at last epoch: 0.692\n",
      "time t = 80\n",
      "training loss at last epoch: 0.689\n",
      "time t = 81\n",
      "training loss at last epoch: 0.689\n",
      "time t = 82\n",
      "training loss at last epoch: 0.689\n",
      "time t = 83\n",
      "training loss at last epoch: 0.694\n",
      "time t = 84\n",
      "training loss at last epoch: 0.692\n",
      "time t = 85\n",
      "training loss at last epoch: 0.694\n",
      "time t = 86\n",
      "training loss at last epoch: 0.697\n",
      "time t = 87\n",
      "training loss at last epoch: 0.696\n",
      "time t = 88\n",
      "training loss at last epoch: 0.696\n",
      "time t = 89\n",
      "training loss at last epoch: 0.695\n",
      "time t = 90\n",
      "training loss at last epoch: 0.696\n",
      "time t = 91\n",
      "training loss at last epoch: 0.695\n",
      "time t = 92\n",
      "training loss at last epoch: 0.692\n",
      "time t = 93\n",
      "training loss at last epoch: 0.693\n",
      "time t = 94\n",
      "training loss at last epoch: 0.693\n",
      "time t = 95\n",
      "training loss at last epoch: 0.691\n",
      "time t = 96\n",
      "training loss at last epoch: 0.691\n",
      "time t = 97\n",
      "training loss at last epoch: 0.693\n",
      "time t = 98\n",
      "training loss at last epoch: 0.692\n",
      "time t = 99\n",
      "training loss at last epoch: 0.691\n",
      "time t = 100\n",
      "training loss at last epoch: 0.695\n",
      "time t = 101\n",
      "training loss at last epoch: 0.688\n",
      "time t = 102\n",
      "training loss at last epoch: 0.690\n",
      "time t = 103\n",
      "training loss at last epoch: 0.690\n",
      "time t = 104\n",
      "training loss at last epoch: 0.689\n",
      "time t = 105\n",
      "training loss at last epoch: 0.693\n",
      "time t = 106\n",
      "training loss at last epoch: 0.692\n",
      "time t = 107\n",
      "training loss at last epoch: 0.695\n",
      "time t = 108\n",
      "training loss at last epoch: 0.695\n",
      "time t = 109\n",
      "training loss at last epoch: 0.696\n",
      "time t = 110\n",
      "training loss at last epoch: 0.693\n",
      "time t = 111\n",
      "training loss at last epoch: 0.696\n",
      "time t = 112\n",
      "training loss at last epoch: 0.697\n",
      "time t = 113\n",
      "training loss at last epoch: 0.695\n",
      "time t = 114\n",
      "training loss at last epoch: 0.695\n",
      "time t = 115\n",
      "training loss at last epoch: 0.696\n",
      "time t = 116\n",
      "training loss at last epoch: 0.698\n",
      "time t = 117\n",
      "training loss at last epoch: 0.696\n",
      "time t = 118\n",
      "training loss at last epoch: 0.696\n",
      "time t = 119\n",
      "training loss at last epoch: 0.695\n",
      "time t = 120\n",
      "training loss at last epoch: 0.695\n",
      "time t = 121\n",
      "training loss at last epoch: 0.695\n",
      "time t = 122\n",
      "training loss at last epoch: 0.697\n",
      "time t = 123\n",
      "training loss at last epoch: 0.696\n",
      "time t = 124\n",
      "training loss at last epoch: 0.696\n",
      "time t = 125\n",
      "training loss at last epoch: 0.694\n",
      "time t = 126\n",
      "training loss at last epoch: 0.696\n",
      "time t = 127\n",
      "training loss at last epoch: 0.696\n",
      "time t = 128\n",
      "training loss at last epoch: 0.699\n",
      "time t = 129\n",
      "training loss at last epoch: 0.697\n",
      "time t = 130\n",
      "training loss at last epoch: 0.695\n",
      "time t = 131\n",
      "training loss at last epoch: 0.699\n",
      "time t = 132\n",
      "training loss at last epoch: 0.697\n",
      "time t = 133\n",
      "training loss at last epoch: 0.698\n",
      "time t = 134\n",
      "training loss at last epoch: 0.695\n",
      "time t = 135\n",
      "training loss at last epoch: 0.693\n",
      "time t = 136\n",
      "training loss at last epoch: 0.693\n",
      "time t = 137\n",
      "training loss at last epoch: 0.693\n",
      "time t = 138\n",
      "training loss at last epoch: 0.691\n",
      "time t = 139\n",
      "training loss at last epoch: 0.691\n",
      "time t = 140\n",
      "training loss at last epoch: 0.691\n",
      "time t = 141\n",
      "training loss at last epoch: 0.691\n",
      "time t = 142\n",
      "training loss at last epoch: 0.693\n",
      "time t = 143\n",
      "training loss at last epoch: 0.695\n",
      "time t = 144\n",
      "training loss at last epoch: 0.700\n",
      "time t = 145\n",
      "training loss at last epoch: 0.695\n",
      "time t = 146\n",
      "training loss at last epoch: 0.697\n",
      "time t = 147\n",
      "training loss at last epoch: 0.694\n",
      "time t = 148\n",
      "training loss at last epoch: 0.696\n",
      "time t = 149\n",
      "training loss at last epoch: 0.697\n",
      "time t = 150\n",
      "training loss at last epoch: 0.696\n",
      "time t = 151\n",
      "training loss at last epoch: 0.694\n",
      "time t = 152\n",
      "training loss at last epoch: 0.695\n",
      "time t = 153\n",
      "training loss at last epoch: 0.696\n",
      "time t = 154\n",
      "training loss at last epoch: 0.698\n",
      "time t = 155\n",
      "training loss at last epoch: 0.696\n",
      "time t = 156\n",
      "training loss at last epoch: 0.698\n",
      "time t = 157\n",
      "training loss at last epoch: 0.696\n",
      "time t = 158\n",
      "training loss at last epoch: 0.697\n",
      "time t = 159\n",
      "training loss at last epoch: 0.699\n",
      "time t = 160\n",
      "training loss at last epoch: 0.696\n",
      "time t = 161\n",
      "training loss at last epoch: 0.696\n",
      "time t = 162\n",
      "training loss at last epoch: 0.694\n",
      "time t = 163\n",
      "training loss at last epoch: 0.697\n",
      "time t = 164\n",
      "training loss at last epoch: 0.697\n",
      "time t = 165\n",
      "training loss at last epoch: 0.695\n",
      "time t = 166\n",
      "training loss at last epoch: 0.694\n",
      "time t = 167\n",
      "training loss at last epoch: 0.695\n",
      "time t = 168\n",
      "training loss at last epoch: 0.696\n",
      "time t = 169\n",
      "training loss at last epoch: 0.696\n",
      "time t = 170\n",
      "training loss at last epoch: 0.697\n",
      "time t = 171\n",
      "training loss at last epoch: 0.696\n",
      "time t = 172\n",
      "training loss at last epoch: 0.696\n",
      "time t = 173\n",
      "training loss at last epoch: 0.700\n",
      "time t = 174\n",
      "training loss at last epoch: 0.695\n",
      "time t = 175\n",
      "training loss at last epoch: 0.693\n",
      "time t = 176\n",
      "training loss at last epoch: 0.693\n",
      "time t = 177\n",
      "training loss at last epoch: 0.692\n",
      "time t = 178\n",
      "training loss at last epoch: 0.694\n",
      "time t = 179\n",
      "training loss at last epoch: 0.691\n",
      "time t = 180\n",
      "training loss at last epoch: 0.689\n",
      "time t = 181\n",
      "training loss at last epoch: 0.692\n",
      "time t = 182\n",
      "training loss at last epoch: 0.692\n",
      "time t = 183\n",
      "training loss at last epoch: 0.694\n",
      "time t = 184\n",
      "training loss at last epoch: 0.690\n",
      "time t = 185\n",
      "training loss at last epoch: 0.693\n",
      "time t = 186\n",
      "training loss at last epoch: 0.691\n",
      "time t = 187\n",
      "training loss at last epoch: 0.694\n",
      "time t = 188\n",
      "training loss at last epoch: 0.696\n",
      "time t = 189\n",
      "training loss at last epoch: 0.692\n",
      "time t = 190\n",
      "training loss at last epoch: 0.705\n",
      "time t = 191\n",
      "training loss at last epoch: 0.697\n",
      "time t = 192\n",
      "training loss at last epoch: 0.694\n",
      "time t = 193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(model, trainloader, testloader, ttestloader, optimizer, device)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m iloss, ploss, _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     67\u001b[0m iloss_list\u001b[38;5;241m.\u001b[39mappend(iloss)\n",
      "Cell \u001b[0;32mIn[81], line 143\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 143\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[81], line 106\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), y)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n\u001b[0;32m--> 106\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep() \n\u001b[1;32m    109\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniforge3/envs/prol/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/prol/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# configure and obtain the data generating process\n",
    "period = 40\n",
    "seq_len = 10000\n",
    "ùúÜ = 5\n",
    "p = 0.0\n",
    "\n",
    "cfg = ProcessConfig(\n",
    "    period=period,\n",
    "    seq_len=seq_len,\n",
    "    num_seeds=3,\n",
    "    ùúÜ=ùúÜ,\n",
    "    p=p\n",
    ")\n",
    "dp = DataGeneratingProcess(cfg)\n",
    "dp.generate_data()\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# learners being considered\n",
    "methods = [\"FTL\", \"Prospective\"]\n",
    "# methods = [\"FTL\", \"Prospective\", \"OGD\", \"BGD\"]\n",
    "\n",
    "t_list = np.arange(20, 1200, 1) # list of time steps \n",
    "\n",
    "instant_risk = {} # store instant risk for each algorithm\n",
    "prospective_risk = {} # store prospective risk for each algorithm\n",
    "\n",
    "for method in methods:\n",
    "    instant_losses = []\n",
    "    prospective_losses = []\n",
    "    for seed in range(cfg.num_seeds):\n",
    "        print(f\"seed = {seed}\")\n",
    "        acorn = seed * 1000 + 1996\n",
    "        set_seed(acorn)\n",
    "\n",
    "        online = (\"OGD\" in method) or (\"BGD\" in method)\n",
    "\n",
    "        if online:\n",
    "            model = MLP(prospective=False)\n",
    "            model.to(device)\n",
    "            optimizer = get_optimizer(model, bgd=True if \"BGD\" in method else False)\n",
    "\n",
    "        iloss_list = []\n",
    "        ploss_list = []\n",
    "        \n",
    "        for t in t_list:\n",
    "            print(f\"time t = {t}\")\n",
    "\n",
    "            if online:\n",
    "                is_first_step = True if t == t_list[0] else False\n",
    "                trainloader, testloader, ttestloader = get_dataloaders(dp, t, seed, past=None if is_first_step else 16)\n",
    "                trainer = Trainer(model, trainloader, testloader, ttestloader, optimizer, device)\n",
    "                if t > 2:\n",
    "                    trainer.train(num_epochs=100 if is_first_step else 1)\n",
    "            else:\n",
    "                trainloader, testloader, ttestloader = get_dataloaders(dp, t, seed)\n",
    "                prospective = True if \"Prospective\" in method else False\n",
    "                model = MLP(prospective=prospective)\n",
    "                model.to(device)\n",
    "                optimizer = get_optimizer(model)\n",
    "                trainer = Trainer(model, trainloader, testloader, ttestloader, optimizer, device)\n",
    "                if t > 2:\n",
    "                    trainer.train(num_epochs=100)\n",
    "                \n",
    "            iloss, ploss, _ = trainer.evaluate()\n",
    "            iloss_list.append(iloss)\n",
    "            ploss_list.append(ploss)\n",
    "\n",
    "        instant_losses.append(iloss_list)\n",
    "        prospective_losses.append(ploss_list)\n",
    "\n",
    "    instant_risk[method] = np.stack(instant_losses).mean(axis=0)\n",
    "    prospective_risk[method] = np.stack(prospective_losses).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pickle\n",
    "results = {\n",
    "    \"prisk\": prospective_risk, \n",
    "    \"irisk\": instant_risk\n",
    "}\n",
    "with open('results.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set(context='poster',\n",
    "            style='ticks',\n",
    "            font_scale=0.9,\n",
    "            rc={'axes.grid':True,\n",
    "                'grid.color':'.9',\n",
    "                'grid.linewidth':0.75})\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n",
    "colors = ['#377eb8', '#e41a1c', '#4daf4a', '#984ea3']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    ax = axes[0]\n",
    "    ax.plot(t_list, instant_risk[method], color=colors[i], lw=2.5, label=method)\n",
    "    ax.set_ylabel(\"Instantaneous Risk\")\n",
    "    ax.set_yticks([0, 0.5, 1])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlim([t_list[0], t_list[-1]])\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(t_list, prospective_risk[method], color=colors[i], lw=2.5)\n",
    "    ax.set_ylabel(\"Prospective Risk\")\n",
    "    ax.set_yticks([0, 0.5, 1])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlim([t_list[0], t_list[-1]])\n",
    "\n",
    "    ax.set_xlabel(\"Time (t)\")\n",
    "\n",
    "switch_times = np.arange(0, t_list[-1]+2, period//2)\n",
    "for i in range(0, len(switch_times)-1, 2):\n",
    "    for j in range(0, 2):\n",
    "        ax = axes[j]\n",
    "        ax.fill_betweenx([-0.1, 1.1], switch_times[i], switch_times[i+1], color='lightgray', alpha=0.2)\n",
    "        \n",
    "for j in range(0, 2):\n",
    "    ax = axes[j]\n",
    "    ax.plot(t_list, np.zeros_like(t_list), lw=2, ls='dashed', color='k', label=\"Bayes Risk\")\n",
    "axes[0].grid(visible=False)\n",
    "axes[1].grid(visible=False)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='k', lw=4, ls='dashed', label='Bayes Risk'),\n",
    "    Line2D([0], [0], color='#377eb8', lw=4, label='Follow-the-Leader'),\n",
    "    Line2D([0], [0], color='#4daf4a', lw=4, label='Online GD'),\n",
    "    Line2D([0], [0], color='#984ea3', lw=4, label='Bayesian GD'),\n",
    "    Line2D([0], [0], color='#e41a1c', lw=4, label='Prospective Learner'),\n",
    "]\n",
    "\n",
    "ax = axes[0]\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 0.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_count(t, seq_len):\n",
    "    import matplotlib.pyplot as plt\n",
    "    sample_count_per_time = [sum(t == i) for i in range(0, seq_len)]\n",
    "    plt.bar(np.arange(0, 1000), sample_count_per_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
