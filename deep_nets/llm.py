import torch
import os
import re
import hydra
import numpy as np

from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.init import set_seed, open_log, init_wandb


class prospectiveLLM:
    def __init__(self, cfg):
        self.dev = cfg.dev
        self.runs = cfg.runs
        self.p = cfg.p
        self.llm = cfg.llm

        if cfg.prob:
            self.scenario = 0
        else:
            self.scenario = cfg.scenario

        self.tokenizer = AutoTokenizer.from_pretrained(cfg.llm)
        self.model = AutoModelForCausalLM.from_pretrained(
            cfg.llm,
            torch_dtype=torch.float16,
            device_map=self.dev)

        torch.compile(self.model, mode="reduce-overhead", fullgraph=True)

    def generate_tokens(self, prompt):
        inp_tokens = self.tokenizer([prompt], return_tensors="pt").to(self.dev)
        ln = inp_tokens['input_ids'].shape[1]

        generated_ids = self.model.generate(
            **inp_tokens,
            do_sample=False,
            top_p=None,
            temperature=None,
            use_cache=True, # Use-kv cache
            max_new_tokens=20)[:, ln:]

        gen_tokens = self.tokenizer.batch_decode(generated_ids)[0]
        gen_tokens = gen_tokens.replace('\n', '\n\n')
        numbers = re.findall(r'\d+', gen_tokens)
        err = self.calc_err(numbers[-1])
        return err

    def scenario0_prompt(self):
        prefix = "Generate outcomes of 10 Bernoulli trials where 0 is generated with probability 0.75 and 1 with probability 0.25\n\n"

        prompts = []
        for i in range(1024):
            s = format(i, '010b')
            prompts.append(prefix + s)

        return prompts

    def scenario1_prompt(self, n):
        txt1 = "Consider the following sequence of outcomes generated from a single Bernoulli distribution\n\n"
        txt2 = "\n\n the next 20 most likely outcomes are:\n\n"

        # Generate sequeunce of outcomes
        generate_coinflips = []
        for i in range(n):
            coinflip = "1" if np.random.uniform(0, 1) < self.p else "0"
            generate_coinflips.append(coinflip)
        generate_coinflips = "".join(generate_coinflips)
        prompt = txt1 + generate_coinflips + txt2

        return prompt
    
    def scenario2_prompt(self, n):
        txt1 = "Consider the following sequence of outcomes generated by two Bernoulli distributions, where all even outcomes are generated by a Bernoulli distribution with parameter 'p' and odd outcomes are generated from a Bernoulli distribution with parameter '1-p'\n\n"
        txt2 = "\n\n The next 20 most likely sequence of outcomes are:\n\n"

        # Generate sequeunce of outcomes
        generate_coinflips = []
        for i in range(n):
            if i % 2 == 0:
                coinflip = "1" if np.random.uniform(0, 1) < self.p else "0"
            else:
                coinflip = "0" if np.random.uniform(0, 1) < self.p else "1"
            generate_coinflips.append(coinflip)
        generate_coinflips = "".join(generate_coinflips)

        prompt = txt1 + generate_coinflips + txt2

        return prompt

    def scenario3_prompt(self, n):
        txt1 = "Consider the following sequence of states generated by a Markov process with 2 states (0, 1):\n\n"
        txt2 = "\n\nThe next 20 most likely sequence of states are:\n\n"

        # Generate sequeunce of outcomes
        markov_txt, targets = [], []
        transition = [[1 - self.p, self.p],
                      [self.p, 1 - self.p]]
        state = 0
        for i in range(n+20):
            if i < n:
                markov_txt.append(str(state))
            else:
                targets.append(state)
            state = np.random.choice([0, 1], p=transition[state])

        markov_txt = "".join(markov_txt)
        prompt = txt1 + markov_txt + txt2
        self.targets = targets
        return prompt 

    def prob_scenario0(self):
        prompts = self.scenario0_prompt()

        all_log_prob = []

        for pr in prompts:
            inp_tokens = self.tokenizer([pr], return_tensors="pt")

            rvs = pr.split("\n\n")[1]
            if "llama" in self.llm:
                rv_tokens = self.tokenizer.encode(rvs)[2:]
            else:
                rv_tokens = self.tokenizer.encode(rvs)[1:]

            tok_ln = len(inp_tokens["input_ids"][0])
            rv_ln = len(rv_tokens)

            logits = self.model(inp_tokens['input_ids'].to(self.dev), return_dict=True).logits
            logits = logits[0, tok_ln - rv_ln - 1:, :]
            logits = logits[:-1]

            ind0 = self.tokenizer.convert_tokens_to_ids(["0"])[0]
            ind1 = self.tokenizer.convert_tokens_to_ids(["1"])[0]

            # delete all other logits
            logits = logits[:, [ind0, ind1]]
            logits = torch.nn.functional.softmax(logits, dim=-1)

            log_prob_list = []
            for i, tok in enumerate(rv_tokens):
                st = self.tokenizer.decode(tok)

                if st == "0" or st == "1":
                    log_prob = np.log(logits[i, int(st)].item())
                else:
                    log_prob = 0

                log_prob_list.append(log_prob)
            all_log_prob.append(log_prob_list)
        all_log_prob = np.array(all_log_prob)

        mname = self.llm.split("/")[1]
        np.save("data/llm/gen_probs_%s.npy" % mname, all_log_prob)

    def err_scenario1(self, numbers):
        errs = []
        for ni in numbers:
            if ni == "1" or ni == "0":
                er = 1-self.p if ni == "1" else self.p
                errs.append(er)
        return np.mean(errs)
    
    def err_scenario2(self, numbers):
        maxidx, maxval = -1, -1
        for idx, ne in enumerate(numbers):
            if len(ne) > maxval:
                maxval = max(maxval, len(ne))
                maxidx = idx

        errs = []
        for idx, ni in enumerate(numbers[maxidx]):
            if ni == "1" or ni == "0":
                err = 1 - self.p if ni == "1" else self.p
                if idx % 2 == 1:
                    err = 1 - err
                errs.append(err)
        return np.mean(errs)

    def err_scenario3(self, numbers):
        maxidx, maxval = -1, -1
        for idx, ne in enumerate(numbers):
            if len(ne) > maxval:
                maxval = max(maxval, len(ne))
                maxidx = idx

        errs = []
        for idx, ni in enumerate(numbers[maxidx]):
            if ni == "1" or ni == "0":
                err = int(self.targets[idx] != int(ni)) * ((0.9) ** (idx))
                errs.append(err)
        return np.mean(errs)

    def fetch_prompt(self, n):
        if self.scenario == 0:
            return self.scenario0_prompt(n)
        elif self.scenario == 1:
            return self.scenario1_prompt(n)
        elif self.scenario == 2:
            return self.scenario2_prompt(n)
        elif self.scenario == 3:
            return self.scenario3_prompt(n)

    def calc_err(self, numbers):
        if self.scenario == 1:
            return self.err_scenario1(numbers)
        elif self.scenario == 2:
            return self.err_scenario2(numbers)
        elif self.scenario == 3:
            return self.err_scenario3(numbers)
        
    def compute_errs(self):
        if self.scenario == 0:
            return self.prob_scenario0()

        all_errs = []
        for n in range(2, 100, 5):
            cur_errs = []
            for seed in range(self.runs):
                prompt = self.fetch_prompt(n)
                err = self.generate_tokens(prompt)
                cur_errs.append((n, err))

            all_errs.append(cur_errs)

        # store all_errs
        mname = self.llm.split("/")[1]
        os.makedirs("data/llm", exist_ok=True)
        np.save("data/llm/scenario%d_%s.npy" % (self.scenario, mname), all_errs)


@hydra.main(config_path="./config/train", config_name="conf.yaml", version_base="1.3")
def main(cfg):
    init_wandb(cfg, project_name="prospective")
    set_seed(cfg.seed)
    open_log(cfg)

    model = prospectiveLLM(cfg)
    model.compute_errs()
